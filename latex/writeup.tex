\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
%\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz,pgfplots,adjustbox,graphicx}
\usepackage{subcaption}
\usepackage{todonotes}
\usepackage{float}
\usepackage{wrapfig}
\definecolor{crimson}{rgb}{0.6,0,0}
\definecolor{red}{HTML}{FF4136}
\definecolor{green}{HTML}{2ECC40}
\setlength{\parindent}{1cm}
\numberwithin{figure}{section}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09

\usepackage{xcolor}
\definecolor{nicepurple}{HTML}{B10DC9}
\definecolor{niceblue}{HTML}{0074D9}
\usepackage[colorlinks=true,linkcolor=niceblue,citecolor=nicepurple]{hyperref}

\title{Variational Auto Encoder Latent Space Activity and Visualization}


\author{
Jesse Bettencourt\\
Department of Mathematics\\
University of Toronto\\
Toronto, ON \\
\texttt{jessbett@math.toronto.edu} \\
\And
Matt Craddock\\
Department of Computer Science\\
University of Toronto\\
Toronto, ON \\
\texttt{matt.craddock@mail.utoronto.ca} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\renewcommand{\L}{\mathcal{L}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}
\maketitle

\begin{abstract}
In this project we implement a Variational Auto Encoder (VAE) in Tensorflow. We compare experimental results for modifications to the VAE, including Importance Weighting, Batch Normalization, and Warm-Up. In particular, we consider how these additional methods affect the training rate and latent dimension activity. Finally, we demonstrate latent visualization techniques available with VAEs.
\end{abstract}

\section{Introduction to Variational Autoencoders}

Variational Auto Encoders (VAEs) were introduced by Kingma \& Welling 2013 as generative analogues to the standard deterministic auto encoder \cite{Kingma2013}. As with deterministic auto encoders, VAEs pair a bottom-up inference network called an encoder with a top-down generative network called a decoder. 
\par VAEs employ a probabilistic interpretation of these encoder and decoder networks. We assume that our data set $\{x^{(i)}\}_{i=1}^N$ is composed of $N$ i.i.d. samples of some variable $x$. Further, we assume that the data were generated by a random process with continuous latent variable $z$. So we have that our data $x$ was generated by some conditional distribution $p_\theta(x|z)$, where $p_\theta$ is a probability distribution with parameters $\theta$. This provides a probabilistic interpretation of the decoder network, where given a latent variable or ``code`` $z$ we generate a sample $x$ in the data space. Similarly, the role of the encoder would be to take a sample $x$ from data space and give us a latent $z$ sampled from the posterior density distribution $p_\theta(z|x)$. 
\par However, this is where problems arise in the probabilistic interpretation. It is common that the posterior density distribution $p_\theta(z|x)$ is intractable. In order to learn an encoder-decoder network pair, VAEs instead learn a different inference model, $q_\phi(z|x)$, which approximates the true, intractable posterior distribution. Note that our approximate inference model, the encoder distribution, has parameters $\phi$ distinct from the $\theta$ of the true posterior and decoder network. Training a VAE will amount to jointly learning these these parameters.
\par Variational auto encoders are further characterized by their training criterion. Instead of learning an often intractable log-likelihood, the training objective $\L(x)$ is a tractable lower bound to the log-likelihood:


\begin{align}
  \log p_\theta(x)\geq \mathbb{E}_{q_\phi(z|x)}\left[\log\frac{p_\theta(x,z)}{q_\phi(z|x)}\right]=-\L(x)\\
  \L(x)=D_{KL}\left(q_\phi(z|x)||p_\theta(z)\right)-\mathbb{E}_{q_\phi(z|x)}\left[\log{p_\theta(x|z)}\right]
  \label{eq:VAEOBJECTIVE}
\end{align}
 
 Where $D_{KL}$ is the Kullback-Leibler divergence. It will be useful later to directly identify the two components of our objective. The reconstruction error term $\mathbb{E}_{q_\phi(z|x)}\left[\log{p_\theta(x|z)}\right]$ is present in deterministic auto encoders, and represents the likelihood that the input data would be reconstructed by the model. The variational regularization term $D_{KL}\left(q_\phi(z|x)||p_\theta(z)\right)$ represents the KL-divergence between the encoder-induced latent distribution and the true prior on the latent distribution. This term encourages our approximate posterior $q_\phi(z|x)$ to be close to $p_\theta(z)$.
\par Finally, one last detail to discuss in the process of training a VAE is the reparametrization trick. Since the reconstruction error term is estimated by sampling $z\sim q_\phi(z|x)$, using gradient training methods through the sampling process is problematic. To address this, Kingma \& Welling describe an alternative method for generating the samples, simply to let $z=g_\phi(x,\epsilon)$ be a deterministic function of $\phi$ and $\epsilon$ be some independent noise.
\par For example, in our implementation, which we will soon describe in detail, we assume that the true posterior distribution can be approximated by a multivariate Gaussian with diagonal covariance. Therefore we let $q_\phi(z|x)=\mathcal{N}(z;\mu,\sigma^2 I)$. So the outputs of our encoder network are the $\mu$ and $\sigma$ of our approximate posterior. Now in order to train with gradient methods, we reparameterize as described, letting $z=\mu + \sigma\epsilon$ where $\epsilon \sim \mathcal{N}(0,1)$.

\section{Implementing Variational Autoencoders}
%In this section we will detail methods for VAE implementation and their effect on the learning rate.\\ \todo{Is this necessary?}
\subsection{Network Architecture}
Each of the following methods were included into what will we call the `Vanilla VAE'. The network architecture for the Vanilla VAE and all additional methods is similar to the architecture introduced in the Section $3$ example of Auto-Encoding Variational Bayes \cite{Kingma2013}.
\par The encoder and decoder networks are symmetric, shallow, fully-connected neural networks, namely Multi Layer Perceptrons (MLP). Both feature two deterministic layers each with $200$ dimensions (or nodes) per layer. A stochastic, or latent, layer with dimensions $n_z$ receives the output from the final deterministic layer.  In the nodes of the hidden layers the activation is determined by the softplus function, $\ln{\left(1+e^x\right)}$.
\par Practically, the probabilistic encoder network takes data from the input space and encodes a representation into a latent space with dimension $n_z$. In particular, the latent representation, $q_\phi(z|x)=\mathcal{N}(z;\mu,\sigma^2 I)$ is a Gaussian distribution over the possible latent values of $z$ from which data $x$ could have been generated. The probabilistic decoder network takes a latent representation and produces a distribution ${p_\theta(x\mid z)}$ over possible data values $x$ generated by $z$.
\par We implement this network and the following additional methods in Tensorflow. Our implementation follows from examples in Tensorflow and Theano \cite{Sonderby2016,Metzen2015,Ahuja}. In all examples we are training on the MNIST handwritten digit data set. We learn the MLP weights and bias parameters, representing the $\phi$ and $\theta$ distribution parameters, with Adam optimization minimizing $\L(x)$ with parameters $\beta_1 = 0.9, \beta_2 = 0.9, \epsilon = 10^{-4}$ a batch size of $100$, and learning rate of $0.001$, trained for $300$ epochs.


\subsection{Xavier Initialization}
All parameters in the MLP were initialized with the Xavier-Glorot method outlined in \cite{Glorot2010}. Xavier-Glorot initialization is shown to improve learning in deep networks by establishing an effective range for initial values. In general, especially for deep networks, there is an initial value trade-off. If the initial weights are too small then the signal magnitude will decrease through the layers and and the influence will tend too small to be useful. If the weights are too large then the signal will grow as it passes through the layers and will tend to be too large to be representative.
\par Xavier-Glorot initialization addresses this trade-off by sampling initialization weights from a Gaussian distribution with zero mean and variance as a function of the network connections for the node. The variance for weight $w$  of a neuron is given as a function of the number of neurons feeding into it $n_{in}$ and the number of neurons the result feeds to $n_{out}$. The function is defined to be $\text{Var}(w) = \frac{2}{n_{in}+n_{out}}$.


\subsection{Decoder Distribution}

As mentioned previously, the probabilistic decoder network takes a latent representation $z$ and produces a distribution over possible data values, ${p_\theta(x\mid z)}$. In our initial description of the network architecture we specified that output of the encoder MLP is Gaussian, but we made no specification to the decoder output distribution.
\par Two choices for decoder distributions, Gaussian and Bernoulli, were outlined in \textit{Auto-Encoding Variational Bayes}, where the authors suggest that the choice of preferred decoder distribution depends on the type of data \cite{Kingma2013}. 

\subsubsection{Gaussian Decoder and Encoder Structure}
For continuous, real-valued data, Kingma \& Welling suggest letting ${p_\theta(x\mid z)}$ be a multivariate Gaussian distribution. This gives the following structure for the decoder distribution with two hidden deterministic layers $h_1$ and $h_2$:

\begin{align*}
    \log p_\theta(x\mid z) &= \log \mathcal{N}(x; \mu, \sigma^2 I)\\
    \mu &= \text{sigmoid}(W_\mu h_2 + b_\mu)\\
    \log \sigma^2 &= \text{tanh}(W_\sigma h_2+ b_\sigma)\\
    h_2 &= \text{softplus}(W_{2} h_1 + b_{2})\\
    h_1 &= \text{softplus}(W_{1} z + b_{1})
\end{align*}

Note that the parameters $\{W_1,W_2,W_\mu,W_\sigma,b_1,b_2,b_\mu,b_\sigma\}$ here are the learned parameters of the decoder MLP, and represent the decoder distribution parameter, $\theta$ in $p_\theta(x\mid z)$. Further, since our encoder distribution is always a multivariate Gaussian distribution, we use this structure for the encoder, where the $z$ and $x$ are swapped and the weights and biases represent the encoder distribution parameter, $\phi$ in ${q_\phi(z \mid x)}$.

\subsubsection{Bernoulli Decoder Structure}
For binary data, Kingma \& Welling suggest letting ${p_\theta(x\mid z)}$ be a multivariate Bernoulli distribution. This gives the following structure for our decoder distribution with two hidden deterministic layers $h_1$ and $h_2$:

\begin{align*}
    \log p_\theta(x\mid z) &= \sum_{i=1}^D x_i \log y_i + (1-x_i) \cdot \log (1-y_i)\\
    y&=\text{sigmoid}(W_\mu h_2 + b_\mu)\\
    h_2&= \text{softplus}(W_2 h_1 +b_2)\\
    h_1 &= \text{softplus}(W_1 z + b_1)\\
\end{align*}

Again, here the Bernoulli decoder distribution parameter $\theta$ is represented by the learned MLP weights and biases $\theta = \{W_1,W_2,W_\mu,b_1,b_2,b_\mu\}$.

\subsubsection{Comparing Decoder Distributions}
Our findings support the recommendation by Kingma \& Welling that Bernoulli decoder distribution performs better on binary data than a Gaussian decoder. The one-hot MNIST data used to train our VAE is binary, and thus we expected that the Bernoulli decoder would out-preform the Gaussian decoder. The results of this experiment can be found in Figure \ref{fig:bergau}. As expected, the Bernoulli distribution was able to learn a much tighter lower bound on the log-likelihood. It is important to note that VAE with the Gaussian decoder has converged to its lower bound, suggesting that even with longer training it would not learn a better model than the VAE with Bernoulli decoder distribution. Therefore, when implementing VAEs it is important to choose a decoder distribution that is representative of the data type. Note that all further methods were implemented on VAEs with Bernoulli decoder distributions.
\begin{wrapfigure}{l}{0.5\textwidth}
  \captionsetup[subfigure]{justification=centering}
  \resizebox{\linewidth}{!}{\input{"../plots/img/Comparison of Bernoulli and Gaussian.tex"}}
  \caption{Bernoulli v.s Gaussian decoder distributions}
  \label{fig:bergau}
\end{wrapfigure}

\subsection{Importance Weighting}
In their 2015 publication \textit{Importance Weighted Autoencoders}, Burda, Grosse, and Salakhutdinov observed that Kingma \& Welling's $\L(x)$ lower bound on the log-likelihood from Eq.\ref{eq:VAEOBJECTIVE} made strong assumptions about the posterior inference, leading to overly simplified representations. They proposed an improvement called Importance Weighted Auto Encoders (IWAE), a generative model using the VAE network architecture with a few key improvements to model generalizability.
\par The critical feature of an IWAE is that the encoder network uses multiple importance weighted samples to approximate the posterior, where the original VAE uses a single sample. This allows IWAE to approximate complex posteriors which are not available under the stronger VAE posterior assumptions.
\par By considering multiple importance weighted samples we introduce a new lower bound on the log-likelihood which is strictly tighter than Eq.\ref{eq:VAEOBJECTIVE}.
\par Given $K$ independent samples $\{z_1,\mathellipsis,z_K\}$ from the encoder distribution $z_k\sim q_\phi(z|x)$ we define the new lower bound given by the $K$-sample importance weighting expectation of the log-likelihood:
\begin{equation}
   \L_K(x) = \mathbb{E}_{z_1,\mathellipsis,z_K \sim q_\phi(z|x)}\left[\log\frac{1}{K}\sum_{k=1}^{K}\frac{p_\theta(x,z_k)}{q_\phi(z_k|x)}\right]
\end{equation}
\begin{wrapfigure}{l}{0.5\textwidth}
  \captionsetup[subfigure]{justification=centering}
  \resizebox{\linewidth}{!}{\input{"../plots/img/Comparison of Vanilla and Importance Weighted.tex"}}
  \caption{Variational v.s. Importance Weighted (IW)}
  \label{fig:variw}
\end{wrapfigure}
The term inside the sum is the normalized importance weights for the joint distribution. Note in particular that the case $K=1$ corresponds exactly to Kingma \& Welling's $\L(x)$ from Eq.\ref{eq:VAEOBJECTIVE}. Burda, Grosse, and Salakhutdinov show that this importance weighted lower bound is strictly tighter than the vanilla lower bound. In particular, that $\log p(x) \geq \L_{K+1}(x)>\L_K(x)$ Our implementation of IWAE with this lower bound supports their findings. We observed that with $K=5$ samples IWAE significantly improves the learned log-likelihood bound during training. The results of this experiment can be found in Figure \ref{fig:variw}. Since importance weighting so significantly improved the performance of our Auto Encoder, all further methods were implemented on VAEs with importance weighting.
\subsection{Batch Normalization}
Batch normalization is a recent method developed to improve stability and convergence speed in deep networks \cite{Ioffe2015}. In the recent paper \textit{How to Train Deep Variational Autoencoders and Probabilistic Ladder Networks} by S\o nderby et al. 2016, the authors show that batch normalization is an important method for learning deep VAEs \cite{Sonderby2016}. That is, deep generative models with several latent layers. Though we are only considering a shallow model with a single latent layer, we were interested to see how batch normalization affected training as well as latent space activity (discussed in later sections).
\begin{wrapfigure}{l}{0.5\textwidth}
    \resizebox{\linewidth}{!}{\input{"../plots/img/Comparison of Importance Weighted and Batch Normalized.tex"}}
    \caption{Effect of Batch Normalization (BN)}
    \label{fig:bncost}
\end{wrapfigure}
\par Batch normalization was developed to improve learning stability during deep network training. As parameters change during learning the layer output distributions change for each hidden layer, requiring later layers to adjust parameters in response to these distribution changes. The problem that batch normalization attempts to address is that changes in early layer output distributions can cause noisy changes to later layers. 
\par Batch normalization functions by normalizing the inputs of the activation function for each layer so that the inputs across each training batch have a mean of 0 and a variance of 1. However, batch normalization restricts the representation of each layer by assuming this normal distribution. To alleviate some of this restriction, batch normalization introduces learnable parameters to scale the variance of the normal distribution, $\gamma$ and shift the mean, $\beta$. Therefore, we transform each activation function input, $x_i$, with the batch normalization given by:

\begin{align*}
    \text{BN}(x_i)=\gamma\left(\frac{x_i-\mu_B}{\sqrt{\sigma_B^2+\epsilon}}\right)+\beta\\
\end{align*} 
where $\mu_B$ and $\sigma_B$ are the mean and standard deviation of the layer's activation function input across the batch, $\epsilon$ is a small constant addition to the variance to avoid division by 0, and $\gamma$ and $\beta$ are learnable scale and shift parameters. Note in particular that when learning with batch normalization, the learned shift parameter $\beta$ for each layer replaces the need for bias terms added to that layer's activation inputs.
\begin{wrapfigure}{l}{0.5\textwidth}
    \resizebox{\linewidth}{!}{\input{"../plots/img/Comparison of Importance Weighted and Warmed-Up.tex"}}
    \caption{Effect of Warm-Up}
    \label{fig:wucost}
\end{wrapfigure}
\par To train our model we add batch normalization before the activation for all layers in our auto-encoder except the output layers, as described in \cite{Sonderby2016}. See the result of training with batch normalization in Figure \ref{fig:bncost}. Our experiments resulted in VAEs trained with batch normalization performing poorer than VAEs without batch normalization. While the literature suggests that batch normalization significantly improves training for deep networks, we found no improvements to our shallow VAEs. We suggest that the reason for this result is that the benefit of batch normalization is to stabilize the propagation of signal throughout the layers of a deep network. However, to achieve this stabilization, batch normalization imposes assumptions on the distribution of activation inputs for each layer. We suggest that batch normalization reduced performance on our VAEs because they were too shallow to receive any benefit from signal stabilization, and were made less effective by the normalization assumption.


\subsection{Warm Up}
Recall that the log-likelihood lower bound in Eq.\ref{eq:VAEOBJECTIVE} contains a reconstruction term and a variational regularization term. Further, notice that without that variational regularization term the lower bound becomes that of a standard deterministic autoencoder. It has been observed that the variational regularization term causes some latent dimensions to become inactive or `pruned' during training \cite{MacKay2001, Burda2016}. In the later sections of this report we consider the activity of the latent dimensions, and particularly how training prunes or preserves latent dimensions.
\par Pruning non-informative dimensions later in training could be considered advantageous for automatic relevance determination. However, if latent dimensions are pruned too early in training they will not have a chance to learn informative representations. Once the dimensions become inactive in training, they are unlikely be reactivated. This problem of early latent dimension pruning is particularly troublesome for deep VAEs, because deep latent layers depend on the shallow latent dimensions in the network. If shallow latent dimensions are pruned early,  deep latent layers will not be able to learn useful representations \cite{Sonderby2016}.
\par To avoid the problem of early pruning due to the variational regularization, we `warm up' our VAE. Warm-up is achieved by initializing the learning process with the objective of a standard deterministic autoencoder, and then linearly introducing the variational regularization. This way the latent dimensions have a chance to learn useful representations as in a deterministic autoencoder before being possibly pruned by variational regularization.\\ We introduce a warm-up parameter $\beta$ to our objective function which increases linearly from 0 to 1 during the first $N_T$ epochs of training:

\begin{equation}
    -\L(x)_T=-\beta D_{KL}\left(q_\phi(z|x)||p_\theta(z)\right)+\mathbb{E}_{q_\phi(z|x)}\left[\log{p_\theta(x|z)}\right]
\end{equation}

Again, note that this causes the first epoch to initialize a standard deterministic autoencoder objective function, then linearly introduce the variational behaviour. Further, observe that after $N_T$ epochs the model becomes a fully variational autoencoder. This warm-up can also be applied to the lower bound objective of the IWAE identically, by linearly scaling the variational regularization term. See the result of training with warm-up of 100 epochs in Figure \ref{fig:wucost}. We observe that VAEs with warmup learn at the same rate and converge to the same lower bound as VAEs without warmup. However, we will discuss later how warm-up improves early latent dimension pruning, as desired. 


\subsection{Latent Space Dimensionality}
\begin{wrapfigure}{l}{0.5\textwidth}
  \resizebox{\linewidth}{!}{\input{"../plots/img/Comparison of Dimension Values for Cost.tex"}}
  \caption{Latent Dimensionality}
  \label{fig:latdimcost}
\end{wrapfigure}
Finally, we were interested in determining how the choice of latent space dimensionality affected the training of our model. As described previously, the number of latent dimensions, $n_z$ is given by the number of nodes in the stochastic layer of the model.
\par To experiment with this parameter, we trained multiple Importance Weighted VAEs with dimensionality ${n_z \in \{2,5,10,20,50,100\}}$. See the effect of dimensionality on training in Figure \ref{fig:latdimcost}. Our experiments show that models with 2 or 5 latent dimensions were not trained as successfully as models with higher latent dimensions. Further, it is interesting to note that all models with $n_z\geq10$ converged to the same log-likelihood bound. This suggests to us that the additional latent dimensionality was not useful for learning latent representations of the MNIST data. In fact, $n_z=10$ converged slightly faster than higher latent dimensional models, further suggesting that the additional dimensions were superfluous to training.
\par We were particularly interested in this question of latent space dimensionality. It is especially suggestive that models with $n_z\geq10$ perform identically, given that there are 10 classes of digits in MNIST. This motivates our inquiry for the next section where we explore usefulness of latent dimensions by describing their activity.

\section{Latent Dimension Activity}
\subsection{Activity Metric}
In the previous sections we discussed latent dimension activity. Here we define it explicitly, using a method described by Burda, Grosse, \& Salakhutdinov \cite{Burda2016}. Given that the distribution parameters of nodes in the latent space combine to form the latent representation of the data, the authors observed that if the parameters remain the same or very similar while encoding  all data in the training set, then the contribution of those nodes and their corresponding latent dimensions to the representation of the data is minor. As such, they defined a measurement of latent dimension activity from the variance of the expected value of the latent distribution across each dimension:
\begin{align}
  Cov_{\textbf{x}}\left(\mathbb{E}_{u~q(u|\textbf{x})}[u]\right)>\epsilon
  \label{eq:ACTIVITYMET}
\end{align}
\par Where $u$ represents a single dimension in the latent space, $q$ is the generative distribution of the encoder network $q_\phi$, and $\epsilon$ is a threshold value. Any dimension whose activity (represented by the left-hand side of the equation) exceeds $\epsilon$ is defined as ``active`` for the purposes of our experiment. In the experiment from \cite{Burda2016}, an activation of $\epsilon=10^{-2}$ was used, but for this experiment a higher threshold of $\epsilon=10^{-1}$ proved to be more useful as many dimensions surpassed this lower threshold with random variation. The effect of the additional methods described previously, averaged over multiple trials with a 50 dimensional VAE, can be seen in the Figure \ref{fig:activities}.
\begin{figure}[h]
\captionsetup[subfigure]{justification=centering}
  \centering
  \begin{subfigure}[t]{0.3\textwidth}
    \resizebox{\linewidth}{!}{\input{"../plots/img/Comparison of Vanilla and Importance Weighted Activity.tex"}}
    \caption{Importance Weighting (IW)}
    \label{fig:iwlat}
  \end{subfigure}
  \begin{subfigure}[t]{0.3\textwidth}
    \resizebox{\linewidth}{!}{\input{"../plots/img/Comparison of Importance Weighted and Batch Normalized for Activity.tex"}}
    \caption{Batch Normalization (BN)}
    \label{fig:bnlat}
  \end{subfigure}
  \begin{subfigure}[t]{0.3\textwidth}
    \resizebox{\linewidth}{!}{\input{"../plots/img/Comparison of Importance Weighted and Warmed-Up for Activity.tex"}}
    \caption{Warm-Up}
    \label{fig:wulat}
  \end{subfigure}
  \caption{Effect of Methods on Latent Dimension Activity}
  \label{fig:activities}
\end{figure}
\subsection{Effects of Methods on Activity}
We observe from our experiments that the vanilla VAE tends to quickly prune latent dimensions, and once inactive those dimensions are not likely to reactivate. The effect of importance weighting seems to smooth the early pruning, as seen in Figure \ref{fig:iwlat}. We also note that importance weighting encourages convergence to fewer latent dimensions than the vanilla VAE. Batch normalization also reduced the rating of early pruning, but conversely encouraged convergence to a greater number of active latent dimensions. Interestingly, this behaviour is suggested as desirable for deep VAEs where increased latent activity is required for training of deeper layers. However, we know from previous results that the batch normalization does not improve training for our shallow VAEs. It is possible that by encouraging more active latent dimensions here, batch normalization may be reducing the effectiveness of the model to learn fewer, more representative dimensions. Finally, we see that warm-up also smooths early pruning over the importance weighted VAE, removing that drastic initial drop in active dimensions. It is interesting to note the pruning of latent dimensions continues smoothly throughout the entire warm-up period of 100 epochs. The number of active dimensions at the end of the warm-up period is near to the final converged value of dimensions. The converged value of active dimensions did not differ with warm-up from the IWVAE.\\
It is interesting to note that with these methods the latent dimensionality tended to converge to $\sim 10$ active dimensions. This result further supports our previous observation that those additional latent dimensions become superfluous to learning. To explore this further we trained multiple VAEs with latent dimension $n_z \in \{2,5,10,20,50,100\}$ and observed the dimensional activity throughout training, see Figure \ref{fig:latdimlat}. Here we see that latent dimensions fewer than 10 were never pruned by training. All models with latent dimensionality greater than 10 had their active dimensions pruned to $\sim 10$ active dimensions. This suggests to us that the true posterior distribution for our dataset is best represented by a $\sim 10$-dimensional latent space. 
 
%It is clear from our experiments that, as training progresses, importance weighting tends to reduce the number of active dimensions, while batch normalization increases it. Warmup was not seen to affect the final dimensionality count, but did accelerate its convergence. The dimension-reducing effect of importance weighting is likely a result of its more accurate loss function, which reduces the capacity for error in the network and forces a more specific lower entropy latent representation of the data. The dimension-increasing effects of batch normalization are likely a result of its tendency to force a distribution on the output of the deterministic layers. This introduces noise in the form of an additional influence on the data that is passed to the latent layer, which it must then compensate for. The effect of warmup is likely a result of the distinct loss function, which punishes frivolous and incorrect information in the latent layer disproportionately in the early training stages.
%\par An interesting result can be seen in Figure \ref{fig:latdimlat}, in which the dimensionality is varied for models trained with importance weighting and neither batch normalization or warmup. As the dimensionality of the model increases, the number of active dimensions as the training converges tends to be constant at $min(D_{init},10)$ where $D_{init}$ is the number of dimensions being trained (the ``initial`` dimensionality of the latent space before pruning). This suggests that a fixed number of dimensions in the latent space may be ideal for representing a particular data set. In this case, 10. This is consistent with the results in Figure \ref{fig:latdimcost}, which indicated that an increase in the dimensionality beyond 10 had little effect on the loss function of the trained model.
\begin{wrapfigure}{l}{0.5\textwidth}
  \resizebox{\linewidth}{!}{\input{"../plots/img/Comparison of Dimension Values for Activity.tex"}}
  \caption{Effect of Latent Dimensionality}
  \label{fig:latdimlat}
\end{wrapfigure}
\subsection{Visualizing Latent Space Activity during Training}
In their research S{\o}nderby et al. \cite{Sonderby2016} put forward a shaded rectangle plot for latent space activation that we have adapted below in Figure \ref{fig:shadeall}. In these plots, the $x$-axis represents training epochs and the $y$-axis represents the index of a dimension, sorted for clarity by initial activity. The shade of a pixel at a each coordinate represents the log of the activation defined in \ref{eq:ACTIVITYMET} for each dimension and epoch. The log-activation is calculated by sampling the variance of the dimensional mean over a random subset of the data. These shade plots provide a more holistic view of the activity than simply plotting the number of dimensions whose activity exceeds the threshold. For examples, in Figures \ref{fig:shadeiw}, \ref{fig:shadebn}, and \ref{fig:shadewu} it is clear that the dimensions labelled as inactive by the threshold measurement are making a nonzero contribution to the output of the latent layer, which may be useful in deep VAEs, and based on the cost functions plotted in Figures \ref{fig:variw}and \ref{fig:wucost} may useful in improving the accuracy of the model.
\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
    \resizebox{\linewidth}{!}{\includegraphics[scale=1]{{../plots/img/trial.50.1.0.0.0.0.Berno}.png}}
    \caption{Vanilla VAE with Bernoulli Decoder Distribution}
    \label{fig:shadevanil}    
  \end{subfigure}
  \begin{subfigure}[t]{0.45\textwidth}
    \resizebox{\linewidth}{!}{\includegraphics[scale=1]{{../plots/img/trial.50.1.0.0.0.1.Berno+IW}.png}}
    \caption{Importance Weighted (IW)}
    \label{fig:shadeiw}
  \end{subfigure}
  \begin{subfigure}[t]{0.45\textwidth}
    \resizebox{\linewidth}{!}{\includegraphics[scale=1]{{../plots/img/trial.50.1.0.1.0.1.Berno+BN+IW}.png}}
    \caption{IW + Batch Normalization (BN)}
    \label{fig:shadebn}
  \end{subfigure}
  \begin{subfigure}[t]{0.45\textwidth}
    \resizebox{\linewidth}{!}{\includegraphics[scale=1]{{../plots/img/trial.50.1.0.0.1.1.Berno+WU+IW}.png}}
    \caption{IW + Warm Up (WU)}
    \label{fig:shadewu}
  \end{subfigure}
  \begin{subfigure}{0.8\textwidth}
    \resizebox{\linewidth}{!}{\includegraphics[scale=1]{{../plots/img/colorbar}.png}}
    \caption*{$\log{\left[Cov_{\textbf{x}}\left(\mathbb{E}_{u~q(u|\textbf{x})}[u]\right)\right]}$}
    \label{fig:shadebar}
  \end{subfigure}
  \caption{Latent Dimension Activity During Training}
  \label{fig:shadeall}
\end{figure}
\section{Latent Space Visualization}
\subsection{Data Reconstruction}
The first and most apparent technique for visualizing the model learned by our VAEs is to generate images of reconstructed input data. In this visualization we give a single test input to our VAE. The test input is first encoded to a representation in the latent space by the encoder network, then decoded to a reconstructed value in the data space. We generated examples of reconstruction visualizations for VAEs with 2 and 50 latent dimensions, see Figure \ref{fig:recon}. As expected, the 50 dimensional VAE was able to learn better representations of every test input than the 2 dimensional VAE. The 2 dimensional VAE performed comparably well on certain input classes, namely digits 1 and 7, but was completely unable to represent other classes, digits 2 and 0. To better understand why our 2 dimensional VAE was unable to learn representations for these classes, we utilize other visualization techniques which illustrate the distribution of the data classes in the latent space.
\subsection{Data Latent Encoding}
This visualization technique utilizes only the encoder network of our VAE. To understand how our data classes are encoded into the latent distribution, we input a subset of our data and plot their code values in latent space. For an example of this visualization with our 2 dimensional VAE, see Figure \ref{fig:latenc}. This visualization method clearly demonstrates which classes the VAE successfully modelled, and which classes were poorly represented. From our previous visualization, we observed that digits 1 and 7 were clearly reconstructed, and the data encoding visualization demonstrates why. Data classes which are clearly distinguished in latent space from other classes were better constructed by our VAE. Notice that the classes for digits 2 and 3 are not distinguished in the latent encoding, this corresponds to the incorrect reconstruction we observed previously.
\begin{figure}
  \centering
  \begin{minipage}[b]{0.45\textwidth}
    \resizebox{\linewidth}{!}{\includegraphics[scale=1]{{../plots/img/vis_recon}.png}}
    \caption{Reconstruction Visualization}
    \label{fig:recon}
  \end{minipage}
  \begin{minipage}[b]{0.45\textwidth}
    \resizebox{\linewidth}{!}{\includegraphics[scale=1]{{../plots/img/vis_scatter2}.png}}
    \caption{Data Encoding Visualization} 
    \label{fig:latenc}
  \end{minipage}
\end{figure}
\subsection{Latent Manifold Lattice Generation}
The final visualization technique utilizes just the decoder network of our VAE. To visualize our learned manifold we choose latent parameters on a lattice of a hyperplane of our latent space. Then we decode the values on our lattice and visualize the lattice in data space. See an example of this visualization in Figure \ref{fig:latlattice}. Notice that this manifold exactly corresponds to the encoding of our data classes into latent space, e.g. 6s at the top and 1s in the lower right. This is an extremely valuable visualization method because it illustrates how model transitions between data classes. Further, it accessibly demonstrates instances of problematic representation, such as the transition from 7s to 9s to 4s. This visualization technique demonstrates clearly how the latent space encodes representations of our data.
\begin{wrapfigure}[22]{l}{0.5\textwidth}
    \resizebox{\linewidth}{!}{\includegraphics[scale=1]{{../plots/img/vis_latent2_1}.png}}
    \caption{Latent Manifold Visualization}
    \label{fig:latlattice}
\end{wrapfigure}
\section{Conclusion}
Our experiments show that the methods designed for improving deep variational auto encoders do not always provide significant improvements for shallow VAEs. Of the implementations we considered, the most important improvements came with correctly choosing the decoder distribution for our data type and optimizing with the importance weighted objective. We observed that additional latent dimensions improve the model up to a certain dimension, $n_z=10$, but further additional dimensions offer no benefit. To clarify this effect we consider the activity of those additional dimensions during training and observe that dimensions above the $10^{\text{th}}$ tend to be deactivated by being rendered unused during training. We assessed the effectiveness of our methods in maintaining active dimensions throughout training, particularly in reducing early pruning. As the literature suggested, implementing these methods did encourage greater dimension activity throughout training, which is an important characteristic when training deep VAEs. However, we found no significant correlation between increased latent dimension activity and improved learning in our shallow VAE. Finally, we demonstrated three visualization techniques for understanding the learned representation of our VAE. The three techniques each demonstrate, respectively, the entire VAE network, the encoder network, and the decoder network. 
\subsection{Further Research}
One direction for further research is to understand why the VAE deactivates dimensions above the $10^{\text{th}}$. It is suggestive since the MNIST data set is composed of exactly 10 classes. We might test this result by learning a subset of the MNIST classes, for example digits 1-5, and observing the effect on latent dimension activity. In general, we would explore the effect of our methods on more complex datasets, namely on continuous data. Finally, we would adjust our implementation to determine the effect of our methods on deep VAEs, confirming results from literature but  requiring more computational resources than were available here.

\bibliographystyle{plain}
\bibliography{bib}

\end{document}