\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Variational Auto Encoder Latent Space Activity and Visualization}


\author{
Jesse Bettencourt\\
Department of Mathematics\\
University of Toronto\\
Toronto, ON \\
\texttt{jessbett@math.toronto.edu} \\
\And
Matt Craddock\\
Department of Computer Science\\
University of Toronto\\
Toronto, ON \\
\texttt{matt.craddock@mail.utoronto.ca} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\renewcommand{\L}{\mathcal{L}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
The abstract paragraph should be indented 1/2~inch (3~picas) on both left and
right-hand margins. Use 10~point type, with a vertical spacing of 11~points.
The word \textbf{Abstract} must be centered, bold, and in point size 12. Two
line spaces precede the abstract. The abstract must be limited to one
paragraph.
\end{abstract}

\section{Introduction to Variational Autoencoders}
Variational auto-encoders (VAEs), as defined by Kingma \& Welling, 2013 [1], are probabilistic implementations of traditional auto-encoders in which neurons in a latent layer are treated not as fixed values but as probability distributions. This is a major boon to semi-supervised learning (Kingma et al., 2014 [2]) as it allows for non-deterministic generation of data with labels to be trained on. During training, the VAE builds a recognition network represented by the function $q(\textbf{z}|\textbf{x};\theta)$ to predict the nodes of a latent space vector $\textbf{z}$ composed of randomly distributed variables, given a data vector $\textbf{x}$ and parameters $\theta$, as well as a generative network $p(\textbf{x}|\textbf{z};\theta)$ to produce the latter given the former. This is done by maximizing the lower bound on the likelihood function given by:
\begin{align}
  L(\textbf{x}^{(i)})=D_{KL}\left(q(\textbf{z}|\textbf{x}^{(i)};\theta)||p(\textbf{z})\right)-\mathbb{E}_{q(\textbf{z}|\textbf{x}^{(i)};\theta)}\left[\log{p(\textbf{x}^{(i)}|\textbf{z};\theta)}\right]
\end{align}
In this equation the expected value term measures the likelihood of the generative network by measuring the log likelihood of a reconstruction of the data, while the KL divergence term measures the likelihood of the recognition network by comparing the recognized posterior probability to the real posterior. The loss function, $-L(\textbf{x}^{(i)})$, is minimized by way of gradient descent, which can be made more computationally efficient by the use of the stochastic Adam algorithm, researched by Kingma and Ba, 2015 [3].
In an intuitive sense, the latent space distribution $q(\textbf{z}|\textbf{x};\theta)$ is a generic representation of the class of the data vector $\textbf{x}$. When the distribution of $\textbf{z}$ is taken to be Gaussian, as it will be in this experiment, the means of the distribution $E_{q}\left[\textbf{z}\right]$ intuitively represent a prototypical element of the class seen in the label. Based on this idea, Burda, Grosse, \& Salakhutdinov, 2016 [4] considered the "activity" of a particular node as:
\begin{align}
  Cov_{\textbf{x}}\left(\mathbb{E}_{u~q(u|\textbf{x})}[u]\right)>\epsilon
\end{align}
In this equation, $u$ is a single dimension of the full latent vector $\textbf{z}$ and $\epsilon$ is a threshold above which the covariance function indicates substantial activity. This measurement, which reflects the variance of the distribution means in a particular dimension, is a measurement of how pronounced the change in distribution is across different classes in the data, where a higher propensity for change represents a high significance in both predicting and generating (that is, characterizing in the latent space) data of a particular class. The research cited suggests that in general most dimensions of a high dimensional latent space are "pruned out" during training. That is, they are not used to store any significant information about the data, and thus are in a sense wasteful. One of the goals of this experiment is to determine whether this spatial inefficiency in the latent space is simply an inconsequential byproduct of the training process or whether it has a significant impact on the effectiveness of the VAE.
A body of research is emerging, such as Johnson, et al. 2016 [5], Burda, Grosse, \& Salakhutdinov, 2016 [4], and Sønderby et al., 2016 [6] to test various additions and modifications to the original definition of VAEs in order to improve the effectiveness of the networks themselves as well as the process of training them. Sønderby et al., 2016 [6] attempted to better utilize the full dimensionality of the latent space by modifying the training process. In the experiment, two features are added to the VAE with the express purpose of preventing nodes with small contributions from being quickly disregarded. The first is batch normalization (proposed by Ioffe \& Szegedy, 2015 [7]), wherein the weights of nodes in the deterministic layers of the VAE are normalized by their first moment. This prevents weights of different nodes in the same layer from becoming strongly divergent, with certain nodes contributing a much larger share of information to successive layers than others due to their increased weights. The second is called warmup, in which a scaling parameter is added to the negative of the likelihood function in Equation (1), producing the modified loss function:
\begin{align}
  -L(\textbf{x}^{(i)})=\mathbb{E}_{q(\textbf{z}|\textbf{x}^{(i)};\theta)}\left[\log{p(\textbf{x}^{(i)}|\textbf{z};\theta)}\right]-\beta D_{KL}\left(q(\textbf{z}|\textbf{x}^{(i)};\theta)||p(\textbf{z})\right)
\end{align}
The value of $\beta$ is initialized to zero, and scaled up linearly over the training epochs so that the loss due to the generative network, which is strongly dependent on the state of the latent space, is not a strong factor in early training stages, and thus nodes in the latent space that have little relevance early in the training process are not prevented from having their weights increased later in the process by virtue of their irrelevance. Rather, what would otherwise be a large loss in the generative network is tolerated early on, giving time for more latent space dimensions to become active over different data vectors. The experiment discovered that these two features had a strong effect on the number of active latent space dimensions, thus increasing the spatial efficiency of the stochastic layer.
With these examples as precedent, we will proceed first with several attempts to increase the spatial efficiency (equivalently, the number of active dimensions) of the latent space using an array of features and modifications added to the canonical implementation of VAEs by Kingma \& Welling, 2013 [1]. We will then conduct an experiment to measure the effect that a change in the value of our activity metric has on the overall performance of the VAE.\\

 

\section{Implementing Variational Autoencoders}
In this section we will detail methods for VAE implementation and effect on learning rate.\\
\subsection{Network Architecture}
Each of the following methods were included into what will we call the `Vanilla VAE'. The network architecture for the Vanilla VAE and all additional methods is similar to the architecture introduced in the Section $3$ example of Auto-Encoding Variational Bayes \cite{Kingma2013}.\\
The encoder and decoder sub-networks are symmetric, simple, fully-connected neural networks, namely Multi Layer Perceptrons (MLP). Both feature two deterministic, or hidden, layers each with $200$ dimensions (or nodes) per layer. A stochastic, or latent, layer with dimensions $n_z$ on top the deterministic layers. In all layers the activation is the softplus function.\\
Practically, the probabilistic encoder network takes data from the input space and encodes a representation into a latent space with dimension $n_z$. In particular, this latent representation, ${q_\phi(z \mid x)}$ is a Gaussian distribution over the possible latent values of $z$ from which data $x$ could have been generated. The probabilistic decoder network takes a latent representation and produces a distribution ${p_\theta(x\mid z)}$ over possible data values $x$ generated by $z$.\\
We implement this network and the following additional methods in Tensorflow. We learn the MLP weights and bias parameters, representing the $\phi$ and $\theta$ distribution parameters, with Adam optimization minimizing $\L(x)$ with parameters $\beta_1 = 0.9, \beta_2 = 0.9, \epsilon = 10^{-4}$ with batch size $100$, learning rate $0.001$, trained for $300$ epochs.


\subsection{Xavier Initialization}
All parameters in the MLP were initialized with the Xavier-Glorot method outlined in \cite{Glorot2010}. Xavier-Glorot initialization is shown to improve learning in deep networks by establishing a reasonable range for initial values. Especially for deep networks there is an initial value trade-off. If the initial weights are too small then the signal will shrink through the layers and and the influence will trend too small to be useless. If the weights are too large then the signal growth through the layers will trend to large to be representative.\\
Xavier-Glorot initialization addresses this trade-off by sampling initialization weights from a Gaussian distribution with zero mean and variance as a function of the network connections for the node. The variance for weight $w$  of a neuron is given as a function of the number of neurons feeding into it $n_{in}$ and the number of neurons the result feeds to $n_{out}$
\begin{equation*}
    \text{Var}(w) = \frac{2}{n_{in}+n_{out}}
\end{equation*}


\subsection{Decoder Distribution}
%FIGURES NEEDED: Gaussian vs Bernoulli Cost Plot


As mentioned previously, the probabilistic decoder network takes a latent representation $z$ and produces a distribution over possible data values, ${p_\theta(x\mid z)}$. In our initial description of the network architecture we specified that output of the encoder MLP is Gaussian, but we made no specification to the decoder output distribution. \\
Two choices for decoder distributions were outlined in \textit{Auto-Encoding Variational Bayes}, where the authors suggest that the choice of preferred decoder distribution depends on the type of data \cite{Kingma2013}. \\

\subsubsection{Gaussian Decoder and Encoder Structure}
For continuous, real-valued data, Kingma \& Welling suggest letting ${p_\theta(x\mid z)}$ be a multivariate Gaussian distribution. This gives the following structure for the decoder distribution with two hidden deterministic layers $h_1$ and $h_2$:

\begin{align*}
    \log p_\theta(x\mid z) & \log \mathcal{N}(x; \mu, \sigma^2 I)\\
    \intertext{where}\\
    \mu &= W_\mu h_2 + b_\mu\\
    \log \sigma^2 &= W_\sigma h_2+ b_\sigma\\
    h_2 &= \text{softplus}(W_{2} h_1 + b_{2})\\
    h_1 &= \text{softplus}(W_{1} z + b_{1})
\end{align*}

Note that the parameters $\{W_1,W_2,W_\mu,W_\sigma,b_1,b_2,b_\mu,b_\sigma\}$ here are the learned parameters of the decoder MLP, and represent the decoder distribution parameter, $\theta$ in $p_\theta(x\mid z)$. Further, since the encoder distribution is always a multivariate Gaussian distribution, we use this structure for the encoder, where the $z$ and $x$ are swapped and the weights and biases represent the encoder distribution parameter, $\phi$ in ${q_\phi(z \mid x)}$.\\

\subsubsection{Bernoulli Decoder Structure}
For binary data, Kingma \& Welling suggest letting ${p_\theta(x\mid z)}$ be a multivariate Bernoulli distribution.

\subsection{Importance Weighting}
%FIGURES NEEDED: Vanilla vs Vanilla + IW cost plot

\subsection{Random Dropout}
%FIGURES NEEDED: Vanilla+IW+Dropout cost plot for all values of keep probability, average across trials for smoothness.


\subsection{Batch Normalization}
%FIGURES NEEDED: Vanilla+IW vs Vanilla+IW+BN cost plot

\subsection{Warm Up}
%FIGURES NEEDED: Vanilla+IW vs Vanilla+IW+WU cost plot

\subsection{Initial Latent Dimension}
%FIGURES NEEDED: Vanilla+IW across initial latet dimension values cost plot

\section{Latent Dimension Activity}
\subsection{Activity Metric}
\subsection{Effects of Implementation on Activity}
\subsubsection{Importance Weighting}
%FIGURES NEEDED: Vanilla vs Vanilla + IW latent line plot and smudge plot

\subsubsection{Random Dropout}
%FIGURES NEEDED: Vanilla+IW+Dropout latent line and smudge plot for all values of keep probability


\subsubsection{Batch Normalization}
%FIGURES NEEDED: Vanilla+IW vs Vanilla+IW+BN latent line and smudge plot

\subsubsection{Warm Up}
%FIGURES NEEDED: Vanilla+IW vs Vanilla+IW+WU latent line and smudge plot

\subsubsection{Initial Latent Dimension}
%FIGURES NEEDED: Vanilla+IW across initial latet dimension values latent line plot


\section{Latent Space Visualization}
\subsection{Data Reconstruction}
%FIGURES NEEDED: reconstruction samples
\subsection{Data Latent Transformation}
%FIGURES NEEDED: Data transformation figure
\subsection{Latent Hyperplane Lattice Generation}
%FIGURES NEEDED: Hyperplane lattice for nz=2, two dimensions with high variance, and bad variance example.


\subsection{References}
\bibliographystyle{plain}
\bibliography{bib}








\end{document}