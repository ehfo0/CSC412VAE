{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST data in a format suited for tensorflow.\n",
    "# The script input_data is available under this URL:\n",
    "# https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/examples/tutorials/mnist/input_data.py\n",
    "import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "n_samples = mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xavier_init(fan_in, fan_out, constant=1): \n",
    "    \"\"\" Xavier initialization of network weights\"\"\"\n",
    "    # haha\n",
    "    low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "    high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), \n",
    "                             minval=low, maxval=high, \n",
    "                             dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(object):\n",
    "    \"\"\" Variation Autoencoder (VAE) with an sklearn-like interface implemented using TensorFlow.\n",
    "    \n",
    "    This implementation uses probabilistic encoders and decoders using Gaussian \n",
    "    distributions and  realized by multi-layer perceptrons. The VAE can be learned\n",
    "    end-to-end.\n",
    "    \n",
    "    See \"Auto-Encoding Variational Bayes\" by Kingma and Welling for more details.\n",
    "    \"\"\"\n",
    "    def __init__(self, network_architecture, transfer_fct=tf.nn.softplus, \n",
    "                 learning_rate=0.001, \n",
    "                 batch_size=100, \n",
    "                 train_keep_prob=1.0, \n",
    "                 warmup=False, \n",
    "                 batch_norm = True,\n",
    "                iw=False):\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.network_architecture = network_architecture\n",
    "        self.transfer_fct = transfer_fct\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.importance_weighting = iw\n",
    "        \n",
    "        self.warmup = warmup\n",
    "        self.batch_norm = batch_norm\n",
    "        self.train_keep_prob = train_keep_prob\n",
    "        \n",
    "        self.epoch = 0.\n",
    "        self.beta = 1.\n",
    "        \n",
    "        # tf Graph input\n",
    "        self.x = tf.placeholder(tf.float32, [None, network_architecture[\"n_input\"]])\n",
    "        \n",
    "        # Keep Probability\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        # Create autoencoder network\n",
    "        self._create_network()\n",
    "        # Define loss function based variational upper-bound and \n",
    "        # corresponding optimizer\n",
    "        #self._create_loss_optimizer()\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(1e-4).minimize(-self._evidence_lower_bound(\\\n",
    "                    importance_weighting=self.importance_weighting))\n",
    "        \n",
    "        self.cost = -self._evidence_lower_bound(\\\n",
    "                    importance_weighting=self.importance_weighting)\n",
    "\n",
    "        \n",
    "        # Initializing the tensor flow variables\n",
    "        init = tf.initialize_all_variables()\n",
    "\n",
    "        # Launch the session\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "    \n",
    "    def _create_network(self):\n",
    "        # Initialize autoencode network weights and biases\n",
    "        self.network_weights = self._initialize_weights(**self.network_architecture)\n",
    "\n",
    "        # Use recognition network to determine mean and \n",
    "        # (log) variance of Gaussian distribution in latent\n",
    "        # space\n",
    "        self.z_mean, self.z_log_sigma_sq = \\\n",
    "            self._recognition_network(self.x,self.network_weights[\"weights_recog\"], \n",
    "                                      self.network_weights[\"biases_recog\"])\n",
    "\n",
    "        # Draw one sample z from Gaussian distribution\n",
    "        n_z = self.network_architecture[\"n_z\"]\n",
    "        eps = tf.random_normal((self.batch_size, n_z), 0, 1, \n",
    "                               dtype=tf.float32)\n",
    "        # z = mu + sigma*epsilon\n",
    "        self.z = tf.add(self.z_mean, \n",
    "                        tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq)), eps))\n",
    "\n",
    "#         # Use generator to determine mean of\n",
    "#         # Bernoulli distribution of reconstructed input\n",
    "#         self.x_reconstr_mean = \\\n",
    "#             self._generator_network(network_weights[\"weights_gener\"],\n",
    "#                                     network_weights[\"biases_gener\"])\n",
    "            \n",
    "            # Use generator to determine mean and (log) variance of\n",
    "        # Gaussian distribution of reconstructed input\n",
    "        self.x_reconstr_mean, self.x_reconstr_log_sigma_sq = \\\n",
    "            self._generator_network(self.z,self.network_weights[\"weights_gener\"],\n",
    "                                   self.network_weights[\"biases_gener\"])\n",
    "            \n",
    "    def _initialize_weights(self, n_hidden_recog_1, n_hidden_recog_2, \n",
    "                            n_hidden_gener_1,  n_hidden_gener_2, \n",
    "                            n_input, n_z):\n",
    "        all_weights = dict()\n",
    "        all_weights['weights_recog'] = {\n",
    "            'h1': tf.Variable(xavier_init(n_input, n_hidden_recog_1)),\n",
    "            'h2': tf.Variable(xavier_init(n_hidden_recog_1, n_hidden_recog_2)),\n",
    "            'out_mean': tf.Variable(xavier_init(n_hidden_recog_2, n_z)),\n",
    "            'out_log_sigma': tf.Variable(xavier_init(n_hidden_recog_2, n_z))}\n",
    "        all_weights['biases_recog'] = {\n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
    "            'out_mean': tf.Variable(tf.zeros([n_z], dtype=tf.float32)),\n",
    "            'out_log_sigma': tf.Variable(tf.zeros([n_z], dtype=tf.float32))}\n",
    "        all_weights['weights_gener'] = {\n",
    "            'h1': tf.Variable(xavier_init(n_z, n_hidden_gener_1)),\n",
    "            'h2': tf.Variable(xavier_init(n_hidden_gener_1, n_hidden_gener_2)),\n",
    "            'out_mean': tf.Variable(xavier_init(n_hidden_gener_2, n_input)),\n",
    "            'out_log_sigma': tf.Variable(xavier_init(n_hidden_gener_2, n_input))}\n",
    "        all_weights['biases_gener'] = {\n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_gener_1], dtype=tf.float32)),\n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_gener_2], dtype=tf.float32)),\n",
    "            'out_mean': tf.Variable(tf.zeros([n_input], dtype=tf.float32)),\n",
    "            'out_log_sigma': tf.Variable(tf.zeros([n_input], dtype=tf.float32))}\n",
    "        return all_weights\n",
    "            \n",
    "    def _recognition_network(self, x, weights, biases):\n",
    "        # Generate probabilistic encoder (recognition network), which\n",
    "        # maps inputs onto a normal distribution in latent space.\n",
    "        # The transformation is parametrized and can be learned.\n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(x, weights['h1']), \n",
    "                                           biases['b1'])) \n",
    "        layer_1 = tf.nn.dropout(layer_1, self.keep_prob)\n",
    "        \n",
    "        if self.batch_norm:\n",
    "            mom_mean1, mom_var1 = tf.nn.moments(layer_1, [0])\n",
    "            layer_1 = tf.nn.batch_normalization(layer_1,mom_mean1,mom_var1,None,None,1E-5)\n",
    "\n",
    "        \n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
    "                                           biases['b2'])) \n",
    "        \n",
    "        layer_2 = tf.nn.dropout(layer_2, self.keep_prob)\n",
    "        \n",
    "        if self.batch_norm:\n",
    "            mom_mean2, mom_var2 = tf.nn.moments(layer_2, [0])\n",
    "            layer_2 = tf.nn.batch_normalization(layer_2,mom_mean2,mom_var2,None,None,1E-5)\n",
    "        \n",
    "        z_mean = tf.add(tf.matmul(layer_2, weights['out_mean']),\n",
    "                        biases['out_mean'])\n",
    "        z_log_sigma_sq = \\\n",
    "            tf.add(tf.matmul(layer_2, weights['out_log_sigma']),\n",
    "                   biases['out_log_sigma'])\n",
    "        return (z_mean, z_log_sigma_sq)\n",
    "\n",
    "    def _generator_network(self, z, weights, biases):\n",
    "        # Generate probabilistic decoder (decoder network), which\n",
    "        # maps points in latent space onto a Bernoulli distribution in data space.\n",
    "        # The transformation is parametrized and can be learned.\n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(z, weights['h1']), \n",
    "                                           biases['b1'])) \n",
    "        \n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
    "                                           biases['b2'])) \n",
    "        \n",
    "        x_reconstr_mean = \\\n",
    "            tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['out_mean']), \n",
    "                                 biases['out_mean']))\n",
    "        x_reconstr_log_sigma_sq = \\\n",
    "            0.5*tf.nn.tanh(tf.add(tf.matmul(layer_2, weights['out_log_sigma']), \n",
    "                                  biases['out_log_sigma']))\n",
    "            \n",
    "        return (x_reconstr_mean, x_reconstr_log_sigma_sq)\n",
    "            \n",
    "    def _create_loss_optimizer(self):\n",
    "        # The loss is composed of two terms:\n",
    "        # 1.) The reconstruction loss (the negative log probability\n",
    "        #     of the input under the reconstructed Bernoulli distribution\n",
    "        #     induced by the decoder in the data space).\n",
    "        #     This can be interpreted as the number of \"nats\" required\n",
    "        #     for reconstructing the input when the activation in latent\n",
    "        #     is given.\\n\",\n",
    "        # Adding 1e-10 to avoid evaluatio of log(0.0)\\n\",\n",
    "        reconstr_loss = \\\n",
    "            -tf.reduce_sum(self.x * tf.log(1e-10 + self.x_reconstr_mean)\n",
    "            + (1-self.x) * tf.log(1e-10 + 1 - self.x_reconstr_mean),\n",
    "            1)\n",
    "        # 2.) The latent loss, which is defined as the Kullback Leibler divergence\n",
    "        #    between the distribution in latent space induced by the encoder on\n",
    "        #     the data and some prior. This acts as a kind of regularizer.\n",
    "        #     This can be interpreted as the number of \"nats\" required\n",
    "        #     for transmitting the the latent space distribution given\n",
    "        #     the prior.\n",
    "        latent_loss = -0.5 * tf.reduce_sum(self.beta * (1 + self.z_log_sigma_sq\n",
    "            - tf.square(self.z_mean)\n",
    "            - tf.exp(self.z_log_sigma_sq)), 1)\n",
    "#         elif self.gen_distribution == 'gaussian':\n",
    "#             #RECONSTRUCTION LOSS GAUSSIAN\n",
    "#             reconstr_loss = \\\n",
    "#                 -tf.reduce_sum(-(0.5 * np.log(2 * np.pi)\n",
    "#                 + self.x_reconstr_log_sigma_sq)\n",
    "#                 - 0.5 * tf.square((self.x - self.x_reconstr_mean)\n",
    "#                 / tf.exp(self.x_reconstr_log_sigma_sq)),\n",
    "#                 1)\n",
    "#             #LATENT LOSS GAUSSIAN\n",
    "#             latent_loss = -0.5 * tf.reduce_sum(1 + 2*self.z_log_sigma_sq\n",
    "#                 - tf.square(self.z_mean)\n",
    "#                 - tf.exp(2*self.z_log_sigma_sq), 1)\n",
    "        # average over batch\n",
    "        self.cost = tf.reduce_mean(reconstr_loss + latent_loss)\n",
    "        # Use ADAM optimizer\n",
    "        self.optimizer = \\\n",
    "            tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "            \n",
    "    \n",
    "    def _evidence_lower_bound(self,\n",
    "                              monte_carlo_samples=5,\n",
    "                              importance_weighting=False,\n",
    "                              tol=1e-5):\n",
    "        \"\"\"\n",
    "            Variational objective function\n",
    "            ELBO = E(log joint log-likelihood) - E(log q)\n",
    "                 = MC estimate of log joint - Entropy(q)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        x_resampled = tf.tile(self.x, tf.constant([monte_carlo_samples, 1]))\n",
    "\n",
    "        # Forward pass of data into latent space\n",
    "        mean_encoder, log_variance_encoder = self._recognition_network(x_resampled,self.network_weights[\"weights_recog\"], \n",
    "                                      self.network_weights[\"biases_recog\"])\n",
    "\n",
    "        random_noise = tf.random_normal(\n",
    "            (self.batch_size * monte_carlo_samples, self.network_architecture['n_z']), 0, 1, dtype=tf.float32)\n",
    "\n",
    "        # Reparameterization trick of re-scaling/transforming random error\n",
    "        std_dev = tf.sqrt(tf.exp(log_variance_encoder))\n",
    "        z = mean_encoder + std_dev * random_noise\n",
    "\n",
    "        # Reconstruction/decoding of latent space\n",
    "        mean_decoder, _ = self._generator_network(z,self.network_weights[\"weights_gener\"],\n",
    "                                   self.network_weights[\"biases_gener\"])\n",
    "\n",
    "        # Bernoulli log-likelihood reconstruction\n",
    "        # TODO: other distributon types\n",
    "        def bernoulli_log_joint(x):\n",
    "            return tf.reduce_sum(\n",
    "                (x * tf.log(tol + mean_decoder))\n",
    "                    + ((1 - x) * tf.log(tol + 1 - mean_decoder)), \n",
    "                1)\n",
    "\n",
    "        log2pi = tf.log(2.0 * np.pi)\n",
    "\n",
    "        def gaussian_likelihood(data, mean, log_variance):\n",
    "            \"\"\"Log-likelihood of data given ~ N(mean, exp(log_variance))\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            data : \n",
    "                Samples from Gaussian centered at mean\n",
    "            mean : \n",
    "                Mean of the Gaussian distribution\n",
    "            log_variance : \n",
    "                Log variance of the Gaussian distribution\n",
    "            Returns\n",
    "            -------\n",
    "            log_likelihood : float\n",
    "            \"\"\"\n",
    "\n",
    "            num_components = data.get_shape().as_list()[1]\n",
    "            variance = tf.exp(log_variance)\n",
    "            log_likelihood = (\n",
    "                -(log2pi * (num_components / 2.0))\n",
    "                - tf.reduce_sum(\n",
    "                    (tf.square(data - mean) / (2 * variance)) + (log_variance / 2.0),\n",
    "                    1)\n",
    "            )\n",
    "\n",
    "            return log_likelihood\n",
    "\n",
    "        def standard_gaussian_likelihood(data):\n",
    "            \"\"\"Log-likelihood of data given ~ N(0, 1)\n",
    "            Parameters\n",
    "            ----------\n",
    "            data : \n",
    "                Samples from Guassian centered at 0\n",
    "            Returns\n",
    "            -------\n",
    "            log_likelihood : float\n",
    "            \"\"\"\n",
    "\n",
    "            num_components = data.get_shape().as_list()[1]\n",
    "            log_likelihood = (\n",
    "                -(log2pi * (num_components / 2.0))\n",
    "                - tf.reduce_sum(tf.square(data) / 2.0, 1)\n",
    "            )\n",
    "\n",
    "            return log_likelihood\n",
    "\n",
    "        log_p_given_z = bernoulli_log_joint(x_resampled)\n",
    "\n",
    "        if importance_weighting:\n",
    "            log_q_z = gaussian_likelihood(z, mean_encoder, log_variance_encoder)\n",
    "            log_p_z = standard_gaussian_likelihood(z)\n",
    "\n",
    "            regularization_term = log_p_z - log_q_z\n",
    "        else:\n",
    "            # Analytic solution to KL(q_z | p_z)\n",
    "            p_z_q_z_kl_divergence = \\\n",
    "                -self.beta*0.5 * tf.reduce_sum(1 \n",
    "                                + log_variance_encoder\n",
    "                                - tf.square(mean_encoder) \n",
    "                                - tf.exp(log_variance_encoder), 1) \n",
    "\n",
    "            regularization_term = -p_z_q_z_kl_divergence\n",
    "\n",
    "        log_p_given_z_mc = tf.reshape(log_p_given_z, \n",
    "                                    [self.batch_size, monte_carlo_samples])\n",
    "        regularization_term_mc = tf.reshape(regularization_term,\n",
    "                            [self.batch_size, monte_carlo_samples])\n",
    "\n",
    "        log_weights = log_p_given_z_mc + regularization_term_mc\n",
    "\n",
    "        if importance_weighting:\n",
    "            # Need to compute normalization constant for weights, which is\n",
    "            # log (sum (exp(log_weights)))\n",
    "            # weights_iw = tf.log(tf.sum(tf.exp(log_weights)))\n",
    "\n",
    "            # Instead using log-sum-exp trick\n",
    "            wmax = tf.reduce_max(log_weights, 1, keep_dims=True)\n",
    "\n",
    "            # w_i = p_x/ q_z, log_wi = log_p_joint - log_qz\n",
    "            # log ( 1/k * sum(exp(log w_i)))\n",
    "            weights_iw = tf.log(tf.reduce_mean(tf.exp(log_weights - wmax), 1))\n",
    "            objective = tf.reduce_mean(wmax) + tf.reduce_mean(weights_iw)\n",
    "        else:\n",
    "            objective = tf.reduce_mean(log_weights)\n",
    "\n",
    "        return objective\n",
    "\n",
    "    \n",
    "    \n",
    "    def partial_fit(self, X, epo):\n",
    "        \"\"\"Train model based on mini-batch of input data.\n",
    "        \n",
    "        Return cost of mini-batch.\n",
    "        \"\"\"\n",
    "        self.epoch= epo\n",
    "        \n",
    "        if self.warmup:\n",
    "            #Warm-up Beta\n",
    "            N_t = 30. #Number of epochs in warmup phase\n",
    "            self.beta=self.epoch/N_t\n",
    "        \n",
    "        \n",
    "        opt, cost = self.sess.run((self.optimizer, self.cost), \n",
    "                                  feed_dict={self.x: X, self.keep_prob: self.train_keep_prob})\n",
    "        return cost\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform data by mapping it into the latent space.\"\"\"\n",
    "        # Note: This maps to mean of distribution, we could alternatively\n",
    "        # sample from Gaussian distribution\n",
    "        return self.sess.run(self.z_mean, feed_dict={self.x: X, self.keep_prob: 1.0})\n",
    "    \n",
    "    def generate(self, z_mu=None):\n",
    "        \"\"\" Generate data by sampling from latent space.\n",
    "        \n",
    "        If z_mu is not None, data for this point in latent space is\n",
    "        generated. Otherwise, z_mu is drawn from prior in latent \n",
    "        space.        \n",
    "        \"\"\"\n",
    "        if z_mu is None:\n",
    "            z_mu = np.random.normal(size=self.network_architecture[\"n_z\"])\n",
    "        # Note: This maps to mean of distribution, we could alternatively\n",
    "        # sample from Gaussian distribution\n",
    "        return self.sess.run(self.x_reconstr_mean, \n",
    "                             feed_dict={self.z: z_mu, self.keep_prob: 1.0})\n",
    "    \n",
    "    def reconstruct(self, X):\n",
    "        \"\"\" Use VAE to reconstruct given data. \"\"\"\n",
    "        return self.sess.run(self.x_reconstr_mean, \n",
    "                             feed_dict={self.x: X, self.keep_prob: 1.0})\n",
    "    \n",
    "def latent_covar(vae,n_samples=mnist.test.num_examples):\n",
    "    test_data, _ =mnist.test.next_batch(n_samples)\n",
    "    z_mean = vae.transform(test_data)\n",
    "    return np.var(z_mean,0)\n",
    "\n",
    "def count_significant(vae,threshold=1e-2,n_samples=mnist.test.num_examples):\n",
    "    return np.greater(latent_covar(vae,n_samples),threshold).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(network_architecture, learning_rate=0.001,\n",
    "            batch_size=100,\n",
    "            training_epochs=10,\n",
    "            display_step=1,\n",
    "            train_keep_prob=1.0,\n",
    "            batch_norm = True,\n",
    "            warmup=False,\n",
    "            iw=False):\n",
    "    vae = VariationalAutoencoder(network_architecture, \n",
    "                                 learning_rate=learning_rate, \n",
    "                                 batch_size=batch_size,\n",
    "                                 train_keep_prob=train_keep_prob,\n",
    "                                 batch_norm = batch_norm,\n",
    "                                 warmup=warmup,\n",
    "                                iw=iw)\n",
    "    \n",
    "    cost_list=np.ndarray(shape=(training_epochs,1),dtype=float)\n",
    "    n_z=network_architecture['n_z']\n",
    "    covar_list=np.ndarray(shape=(training_epochs,n_z),dtype=float)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(n_samples / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, _ = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # Fit training using batch data\n",
    "            cost = vae.partial_fit(batch_xs, epoch)\n",
    "            # Compute average loss\n",
    "            avg_cost += cost / n_samples * batch_size\n",
    "            # Active Latent Dimension Count\n",
    "            count = (count_significant(vae,n_samples=100))\n",
    "            \n",
    "        if np.isnan(avg_cost):\n",
    "            print 'nan'\n",
    "            nan = epoch\n",
    "            break\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \\\n",
    "                  \"cost=\", \"{:.9f}\".format(avg_cost),\\\n",
    "                  \"latent count=\", \"%d\" % count\n",
    "            sys.stdout.flush()\n",
    "        cost_list.itemset((epoch,0),avg_cost)\n",
    "        covari=latent_covar(vae,n_samples=100)\n",
    "        for i in xrange(n_z):\n",
    "            covar_list.itemset((epoch,i),covari[i])\n",
    "        nan = epoch    \n",
    "    return vae, cost_list, covar_list, nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_test(z_dim,keep_prob,b_normal,warmup, iw ,t_epochs=300, trial_num=0):\n",
    "    network_architecture = \\\n",
    "        dict(n_hidden_recog_1=100, # 1st layer encoder neurons\n",
    "             n_hidden_recog_2=100, # 2nd layer encoder neurons\n",
    "             n_hidden_gener_1=100, # 1st layer decoder neurons\n",
    "             n_hidden_gener_2=100, # 2nd layer decoder neurons\n",
    "             n_input=784, # MNIST data input (img shape: 28*28)\n",
    "             n_z=z_dim)  # dimensionality of latent space\n",
    "\n",
    "    vae, cost_list, covar_list, nan = train(network_architecture,learning_rate=0.001,\n",
    "                                        batch_size=100,\n",
    "                                        training_epochs=t_epochs,\n",
    "                                        display_step=10,\n",
    "                                        train_keep_prob=keep_prob,\n",
    "                                        batch_norm = (b_normal==1),\n",
    "                                        warmup = warmup,\n",
    "                                        iw=iw)\n",
    "\n",
    "    namestring='trials_final/{}.{}.{}.{}.{}.trial{}.endedat{}.pkl'.format(z_dim,keep_prob,b_normal,warmup,iw,trial_num,nan)\n",
    "    pickle.dump({'cost':cost_list,'covar':covar_list},open(namestring,'wb'))\n",
    "    vae.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "VANILLA\n",
      "Epoch: 0001 cost= 265.189315380 latent count= 48\n",
      "Epoch: 0011 cost= 142.914030151 latent count= 50\n",
      "Epoch: 0021 cost= 125.487213981 latent count= 49\n",
      "Epoch: 0031 cost= 118.437533292 latent count= 50\n",
      "Epoch: 0041 cost= 114.825090901 latent count= 46\n",
      "Epoch: 0051 cost= 112.591779633 latent count= 37\n",
      "Epoch: 0061 cost= 110.752411693 latent count= 34\n",
      "Epoch: 0071 cost= 109.897258814 latent count= 28\n",
      "Epoch: 0081 cost= 108.963531633 latent count= 31\n",
      "Epoch: 0091 cost= 108.135869474 latent count= 24\n",
      "Epoch: 0101 cost= 107.473314153 latent count= 26\n",
      "Epoch: 0111 cost= 106.961104237 latent count= 26\n",
      "Epoch: 0121 cost= 106.535703291 latent count= 28\n",
      "Epoch: 0131 cost= 106.105572246 latent count= 24\n",
      "Epoch: 0141 cost= 105.988517137 latent count= 25\n",
      "Epoch: 0151 cost= 105.549269326 latent count= 23\n",
      "Epoch: 0161 cost= 105.117968930 latent count= 23\n",
      "Epoch: 0171 cost= 105.047884313 latent count= 23\n",
      "Epoch: 0181 cost= 104.720982666 latent count= 25\n",
      "Epoch: 0191 cost= 104.474392631 latent count= 24\n",
      "Epoch: 0201 cost= 104.282536496 latent count= 20\n",
      "Epoch: 0211 cost= 104.071118969 latent count= 21\n",
      "Epoch: 0221 cost= 103.952628521 latent count= 20\n",
      "Epoch: 0231 cost= 103.823318107 latent count= 21\n",
      "Epoch: 0241 cost= 103.590926944 latent count= 20\n",
      "Epoch: 0251 cost= 103.518403001 latent count= 20\n",
      "Epoch: 0261 cost= 103.399468176 latent count= 20\n",
      "Epoch: 0271 cost= 103.114911083 latent count= 20\n",
      "Epoch: 0281 cost= 103.196876332 latent count= 20\n",
      "Epoch: 0291 cost= 102.980307728 latent count= 20\n",
      "IW\n",
      "Epoch: 0001 cost= 208.855950997 latent count= 41\n",
      "Epoch: 0011 cost= 107.014908641 latent count= 50\n",
      "Epoch: 0021 cost= 92.520194799 latent count= 50\n",
      "Epoch: 0031 cost= 85.785916214 latent count= 50\n",
      "Epoch: 0041 cost= 82.186244632 latent count= 48\n",
      "Epoch: 0051 cost= 80.170049133 latent count= 49\n",
      "Epoch: 0061 cost= 78.221689287 latent count= 48\n",
      "Epoch: 0071 cost= 77.159741176 latent count= 47\n",
      "Epoch: 0081 cost= 76.330008600 latent count= 49\n",
      "Epoch: 0091 cost= 75.449152326 latent count= 47\n",
      "Epoch: 0101 cost= 74.699300461 latent count= 47\n",
      "Epoch: 0111 cost= 74.429256509 latent count= 47\n",
      "Epoch: 0121 cost= 73.899826091 latent count= 47\n",
      "Epoch: 0131 cost= 73.887107280 latent count= 45\n",
      "Epoch: 0141 cost= 73.524902434 latent count= 44\n",
      "Epoch: 0151 cost= 73.321846743 latent count= 42\n",
      "Epoch: 0161 cost= 72.915207270 latent count= 43\n",
      "Epoch: 0171 cost= 72.684820369 latent count= 39\n",
      "Epoch: 0181 cost= 72.502772141 latent count= 34\n",
      "Epoch: 0191 cost= 72.050207936 latent count= 36\n",
      "Epoch: 0201 cost= 71.981549468 latent count= 34\n",
      "Epoch: 0211 cost= 71.698341619 latent count= 35\n",
      "Epoch: 0221 cost= 71.434674176 latent count= 34\n",
      "Epoch: 0231 cost= 71.474316711 latent count= 30\n",
      "Epoch: 0241 cost= 71.389650491 latent count= 29\n",
      "Epoch: 0251 cost= 71.502623582 latent count= 25\n",
      "Epoch: 0261 cost= 70.696593080 latent count= 27\n",
      "Epoch: 0271 cost= 71.022943954 latent count= 26\n",
      "Epoch: 0281 cost= 70.470336352 latent count= 25\n",
      "Epoch: 0291 cost= 70.393153291 latent count= 30\n",
      "WU\n",
      "Epoch: 0001 cost= 211.920771235 latent count= 42\n",
      "Epoch: 0011 cost= 105.528599104 latent count= 50\n",
      "Epoch: 0021 cost= 92.149956443 latent count= 50\n",
      "Epoch: 0031 cost= 86.103995431 latent count= 50\n",
      "Epoch: 0041 cost= 83.014943182 latent count= 50\n",
      "Epoch: 0051 cost= 80.175498255 latent count= 50\n",
      "Epoch: 0061 cost= 77.930261057 latent count= 50\n",
      "Epoch: 0071 cost= 76.731728259 latent count= 45\n",
      "Epoch: 0081 cost= 75.684573628 latent count= 45\n",
      "Epoch: 0091 cost= 75.304265241 latent count= 48\n",
      "Epoch: 0101 cost= 74.366613804 latent count= 45\n",
      "Epoch: 0111 cost= 74.284315130 latent count= 45\n",
      "Epoch: 0121 cost= 73.562613220 latent count= 44\n",
      "Epoch: 0131 cost= 73.289231297 latent count= 41\n",
      "Epoch: 0141 cost= 72.936602520 latent count= 34\n",
      "Epoch: 0151 cost= 72.950671054 latent count= 32\n",
      "Epoch: 0161 cost= 72.425991683 latent count= 27\n",
      "Epoch: 0171 cost= 72.319726084 latent count= 22\n",
      "Epoch: 0181 cost= 71.956689911 latent count= 20\n",
      "Epoch: 0191 cost= 71.907597483 latent count= 18\n",
      "Epoch: 0201 cost= 71.892791387 latent count= 18\n",
      "Epoch: 0211 cost= 71.562569067 latent count= 22\n",
      "Epoch: 0221 cost= 71.021699607 latent count= 12\n",
      "Epoch: 0231 cost= 71.731785597 latent count= 12\n",
      "Epoch: 0241 cost= 71.381540520 latent count= 10\n",
      "Epoch: 0251 cost= 70.937291371 latent count= 12\n",
      "Epoch: 0261 cost= 70.824895505 latent count= 10\n",
      "Epoch: 0271 cost= 70.848182553 latent count= 10\n",
      "Epoch: 0281 cost= 70.560047018 latent count= 10\n",
      "Epoch: 0291 cost= 70.574118666 latent count= 10\n",
      "1\n",
      "VANILLA\n",
      "Epoch: 0001 cost= 269.463207370 latent count= 47\n",
      "Epoch: 0011 cost= 143.667071339 latent count= 50\n",
      "Epoch: 0021 cost= 125.270304649 latent count= 49\n",
      "Epoch: 0031 cost= 118.159238892 latent count= 46\n",
      "Epoch: 0041 cost= 114.607946944 latent count= 46\n",
      "Epoch: 0051 cost= 112.198635448 latent count= 39\n",
      "Epoch: 0061 cost= 110.470134915 latent count= 30\n",
      "Epoch: 0071 cost= 109.224425756 latent count= 25\n",
      "Epoch: 0081 cost= 108.245656461 latent count= 24\n",
      "Epoch: 0091 cost= 107.451686027 latent count= 26\n",
      "Epoch: 0101 cost= 106.914893410 latent count= 26\n",
      "Epoch: 0111 cost= 106.446810109 latent count= 24\n",
      "Epoch: 0121 cost= 105.996732400 latent count= 27\n",
      "Epoch: 0131 cost= 105.628509854 latent count= 22\n",
      "Epoch: 0141 cost= 105.268852331 latent count= 23\n",
      "Epoch: 0151 cost= 104.856532953 latent count= 23\n",
      "Epoch: 0161 cost= 104.641082597 latent count= 23\n",
      "Epoch: 0171 cost= 104.498650041 latent count= 21\n",
      "Epoch: 0181 cost= 104.153292014 latent count= 23\n",
      "Epoch: 0191 cost= 103.984926897 latent count= 21\n",
      "Epoch: 0201 cost= 103.780598575 latent count= 21\n",
      "Epoch: 0211 cost= 103.530426275 latent count= 20\n",
      "Epoch: 0221 cost= 103.412802332 latent count= 20\n",
      "Epoch: 0231 cost= 103.184084930 latent count= 20\n",
      "Epoch: 0241 cost= 103.071846383 latent count= 19\n",
      "Epoch: 0251 cost= 102.988396870 latent count= 19\n",
      "Epoch: 0261 cost= 103.021066298 latent count= 18\n",
      "Epoch: 0271 cost= 102.756586789 latent count= 18\n",
      "Epoch: 0281 cost= 102.543665064 latent count= 18\n",
      "Epoch: 0291 cost= 102.643799563 latent count= 18\n",
      "IW\n",
      "Epoch: 0001 cost= 210.525382163 latent count= 46\n",
      "Epoch: 0011 cost= 105.094037434 latent count= 50\n",
      "Epoch: 0021 cost= 94.291960089 latent count= 50\n",
      "Epoch: 0031 cost= 86.960566073 latent count= 50\n",
      "Epoch: 0041 cost= 82.760657182 latent count= 50\n",
      "Epoch: 0051 cost= 79.510741570 latent count= 50\n",
      "Epoch: 0061 cost= 77.699885316 latent count= 49\n",
      "Epoch: 0071 cost= 76.695199793 latent count= 49\n",
      "Epoch: 0081 cost= 75.614192075 latent count= 48\n",
      "Epoch: 0091 cost= 75.056317673 latent count= 49\n",
      "Epoch: 0101 cost= 74.276025668 latent count= 47\n",
      "Epoch: 0111 cost= 73.796367194 latent count= 46\n",
      "Epoch: 0121 cost= 73.456430054 latent count= 48\n",
      "Epoch: 0131 cost= 73.217793746 latent count= 46\n",
      "Epoch: 0141 cost= 73.425585799 latent count= 45\n",
      "Epoch: 0151 cost= 72.600548512 latent count= 37\n",
      "Epoch: 0161 cost= 72.314305857 latent count= 35\n",
      "Epoch: 0171 cost= 72.381027582 latent count= 32\n",
      "Epoch: 0181 cost= 72.329749423 latent count= 34\n",
      "Epoch: 0191 cost= 71.953141431 latent count= 31\n",
      "Epoch: 0201 cost= 71.568077510 latent count= 31\n",
      "Epoch: 0211 cost= 71.552048520 latent count= 29\n",
      "Epoch: 0221 cost= 71.501958639 latent count= 37\n",
      "Epoch: 0231 cost= 71.257205602 latent count= 33\n",
      "Epoch: 0241 cost= 71.121445347 latent count= 29\n",
      "Epoch: 0251 cost= 71.083329086 latent count= 25\n",
      "Epoch: 0261 cost= 70.867128317 latent count= 27\n",
      "Epoch: 0271 cost= 70.645555219 latent count= 28\n",
      "Epoch: 0281 cost= 70.812953061 latent count= 31\n",
      "Epoch: 0291 cost= 70.593968104 latent count= 28\n",
      "WU\n",
      "Epoch: 0001 cost= 208.702428617 latent count= 42\n",
      "Epoch: 0011 cost= 105.980202845 latent count= 50\n",
      "Epoch: 0021 cost= 95.242546955 latent count= 50\n",
      "Epoch: 0031 cost= 87.135577462 latent count= 50\n",
      "Epoch: 0041 cost= 82.681221598 latent count= 50\n",
      "Epoch: 0051 cost= 79.897376529 latent count= 49\n",
      "Epoch: 0061 cost= 78.400856517 latent count= 49\n",
      "Epoch: 0071 cost= 76.989635932 latent count= 47\n",
      "Epoch: 0081 cost= 76.215282537 latent count= 49\n",
      "Epoch: 0091 cost= 75.391582225 latent count= 46\n",
      "Epoch: 0101 cost= 74.481197433 latent count= 46\n",
      "Epoch: 0111 cost= 73.925190749 latent count= 44\n",
      "Epoch: 0121 cost= 73.220786417 latent count= 47\n",
      "Epoch: 0131 cost= 73.093583943 latent count= 44\n",
      "Epoch: 0141 cost= 72.869452612 latent count= 44\n",
      "Epoch: 0151 cost= 72.804949584 latent count= 42\n",
      "Epoch: 0161 cost= 72.531173900 latent count= 44\n",
      "Epoch: 0171 cost= 72.497378235 latent count= 42\n",
      "Epoch: 0181 cost= 72.396118705 latent count= 39\n",
      "Epoch: 0191 cost= 71.414095098 latent count= 37\n",
      "Epoch: 0201 cost= 71.519961201 latent count= 36\n",
      "Epoch: 0211 cost= 71.318273967 latent count= 36\n",
      "Epoch: 0221 cost= 71.153890263 latent count= 35\n",
      "Epoch: 0231 cost= 71.139300218 latent count= 34\n",
      "Epoch: 0241 cost= 71.343721848 latent count= 32\n",
      "Epoch: 0251 cost= 71.186525407 latent count= 32\n",
      "Epoch: 0261 cost= 71.104395384 latent count= 32\n",
      "Epoch: 0271 cost= 70.533431001 latent count= 32\n",
      "Epoch: 0281 cost= 70.554616623 latent count= 32\n",
      "Epoch: 0291 cost= 70.634336839 latent count= 31\n",
      "2\n",
      "VANILLA\n",
      "Epoch: 0001 cost= 263.911057906 latent count= 46\n",
      "Epoch: 0011 cost= 142.924132968 latent count= 50\n",
      "Epoch: 0021 cost= 125.370981806 latent count= 49\n",
      "Epoch: 0031 cost= 118.607805509 latent count= 46\n",
      "Epoch: 0041 cost= 114.766606986 latent count= 39\n",
      "Epoch: 0051 cost= 112.346413921 latent count= 33\n",
      "Epoch: 0061 cost= 110.819788527 latent count= 31\n",
      "Epoch: 0071 cost= 109.585940247 latent count= 30\n",
      "Epoch: 0081 cost= 108.765222321 latent count= 26\n",
      "Epoch: 0091 cost= 108.016901661 latent count= 18\n",
      "Epoch: 0101 cost= 107.226178214 latent count= 18\n",
      "Epoch: 0111 cost= 106.747157870 latent count= 17\n",
      "Epoch: 0121 cost= 106.389516629 latent count= 17\n",
      "Epoch: 0131 cost= 105.889937189 latent count= 17\n",
      "Epoch: 0141 cost= 105.615537886 latent count= 17\n",
      "Epoch: 0151 cost= 105.142971275 latent count= 17\n",
      "Epoch: 0161 cost= 104.997266513 latent count= 17\n",
      "Epoch: 0171 cost= 104.667991735 latent count= 17\n",
      "Epoch: 0181 cost= 104.512454348 latent count= 17\n",
      "Epoch: 0191 cost= 104.100571511 latent count= 17\n",
      "Epoch: 0201 cost= 104.175664645 latent count= 17\n",
      "Epoch: 0211 cost= 103.963271720 latent count= 17\n",
      "Epoch: 0221 cost= 103.614567913 latent count= 17\n",
      "Epoch: 0231 cost= 103.617301331 latent count= 17\n",
      "Epoch: 0241 cost= 103.570850220 latent count= 17\n",
      "Epoch: 0251 cost= 103.293235501 latent count= 17\n",
      "Epoch: 0261 cost= 103.248284232 latent count= 17\n",
      "Epoch: 0271 cost= 102.934247242 latent count= 17\n",
      "Epoch: 0281 cost= 102.968769601 latent count= 17\n",
      "Epoch: 0291 cost= 102.796692491 latent count= 17\n",
      "IW\n",
      "Epoch: 0001 cost= 212.889503881 latent count= 45\n",
      "Epoch: 0011 cost= 105.667284851 latent count= 50\n",
      "Epoch: 0021 cost= 93.966533425 latent count= 50\n",
      "Epoch: 0031 cost= 86.257643835 latent count= 50\n",
      "Epoch: 0041 cost= 82.376499655 latent count= 50\n",
      "Epoch: 0051 cost= 80.089608314 latent count= 50\n",
      "Epoch: 0061 cost= 78.489130478 latent count= 50\n",
      "Epoch: 0071 cost= 77.103050468 latent count= 48\n",
      "Epoch: 0081 cost= 76.361665351 latent count= 50\n",
      "Epoch: 0091 cost= 75.416685181 latent count= 49\n",
      "Epoch: 0101 cost= 74.774194031 latent count= 50\n",
      "Epoch: 0111 cost= 74.318211205 latent count= 48\n",
      "Epoch: 0121 cost= 74.017518193 latent count= 48\n",
      "Epoch: 0131 cost= 73.546708131 latent count= 48\n",
      "Epoch: 0141 cost= 73.112097598 latent count= 46\n",
      "Epoch: 0151 cost= 73.402458989 latent count= 44\n",
      "Epoch: 0161 cost= 73.030429826 latent count= 44\n",
      "Epoch: 0171 cost= 72.373729408 latent count= 38\n",
      "Epoch: 0181 cost= 72.008425508 latent count= 33\n",
      "Epoch: 0191 cost= 71.688610285 latent count= 33\n",
      "Epoch: 0201 cost= 71.453554424 latent count= 32\n",
      "Epoch: 0211 cost= 71.410368756 latent count= 25\n",
      "Epoch: 0221 cost= 71.425415476 latent count= 21\n",
      "Epoch: 0231 cost= 71.343407454 latent count= 18\n",
      "Epoch: 0241 cost= 71.129505816 latent count= 13\n",
      "Epoch: 0251 cost= 70.709512697 latent count= 11\n",
      "Epoch: 0261 cost= 70.822078039 latent count= 11\n",
      "Epoch: 0271 cost= 70.894583560 latent count= 11\n",
      "Epoch: 0281 cost= 70.716083048 latent count= 11\n",
      "Epoch: 0291 cost= 70.559503798 latent count= 11\n",
      "WU\n",
      "Epoch: 0001 cost= 208.248283414 latent count= 47\n",
      "Epoch: 0011 cost= 107.208491710 latent count= 50\n",
      "Epoch: 0021 cost= 93.210452548 latent count= 50\n",
      "Epoch: 0031 cost= 86.136169718 latent count= 50\n",
      "Epoch: 0041 cost= 81.928753856 latent count= 50\n",
      "Epoch: 0051 cost= 79.669912727 latent count= 50\n",
      "Epoch: 0061 cost= 77.557441815 latent count= 50\n",
      "Epoch: 0071 cost= 76.474237622 latent count= 50\n",
      "Epoch: 0081 cost= 76.016664935 latent count= 47\n",
      "Epoch: 0091 cost= 75.163997865 latent count= 46\n",
      "Epoch: 0101 cost= 74.755686992 latent count= 43\n",
      "Epoch: 0111 cost= 74.144155634 latent count= 43\n",
      "Epoch: 0121 cost= 73.487138901 latent count= 43\n",
      "Epoch: 0131 cost= 73.321301824 latent count= 44\n",
      "Epoch: 0141 cost= 72.837491247 latent count= 44\n",
      "Epoch: 0151 cost= 73.074114012 latent count= 45\n",
      "Epoch: 0161 cost= 72.550750698 latent count= 47\n",
      "Epoch: 0171 cost= 72.319958205 latent count= 48\n",
      "Epoch: 0181 cost= 72.127110055 latent count= 44\n",
      "Epoch: 0191 cost= 71.855630375 latent count= 41\n",
      "Epoch: 0201 cost= 71.684223869 latent count= 40\n",
      "Epoch: 0211 cost= 71.427718541 latent count= 40\n",
      "Epoch: 0221 cost= 71.727862507 latent count= 39\n",
      "Epoch: 0231 cost= 71.115733622 latent count= 37\n",
      "Epoch: 0241 cost= 71.190017000 latent count= 36\n",
      "Epoch: 0251 cost= 70.567013224 latent count= 33\n",
      "Epoch: 0261 cost= 70.908625655 latent count= 33\n",
      "Epoch: 0271 cost= 70.409580002 latent count= 31\n",
      "Epoch: 0281 cost= 70.555244737 latent count= 33\n",
      "Epoch: 0291 cost= 70.911631712 latent count= 30\n",
      "Epoch: 0001 cost= 212.645625180 latent count= 2\n",
      "Epoch: 0011 cost= 110.561585000 latent count= 2\n",
      "Epoch: 0021 cost= 104.510796273 latent count= 2\n",
      "Epoch: 0031 cost= 102.511857203 latent count= 2\n",
      "Epoch: 0041 cost= 101.120341630 latent count= 2\n",
      "Epoch: 0051 cost= 98.842381661 latent count= 2\n",
      "Epoch: 0061 cost= 97.477008431 latent count= 2\n",
      "Epoch: 0071 cost= 95.535137093 latent count= 2\n",
      "Epoch: 0081 cost= 94.532829812 latent count= 2\n",
      "Epoch: 0091 cost= 93.845073131 latent count= 2\n",
      "Epoch: 0101 cost= 93.178931108 latent count= 2\n",
      "Epoch: 0111 cost= 92.995007213 latent count= 2\n",
      "Epoch: 0121 cost= 91.499622664 latent count= 2\n",
      "Epoch: 0131 cost= 91.920426761 latent count= 2\n",
      "Epoch: 0141 cost= 91.053236084 latent count= 2\n",
      "Epoch: 0151 cost= 90.803410450 latent count= 2\n",
      "Epoch: 0161 cost= 90.784175720 latent count= 2\n",
      "Epoch: 0171 cost= 90.871364413 latent count= 2\n",
      "Epoch: 0181 cost= 89.941342434 latent count= 2\n",
      "Epoch: 0191 cost= 90.205840600 latent count= 2\n",
      "Epoch: 0201 cost= 89.449725841 latent count= 2\n",
      "Epoch: 0211 cost= 90.024304754 latent count= 2\n",
      "Epoch: 0221 cost= 89.153413086 latent count= 2\n",
      "Epoch: 0231 cost= 89.015174422 latent count= 2\n",
      "Epoch: 0241 cost= 88.767364863 latent count= 2\n",
      "Epoch: 0251 cost= 87.853675523 latent count= 2\n",
      "Epoch: 0261 cost= 87.899311891 latent count= 2\n",
      "Epoch: 0271 cost= 87.834688069 latent count= 2\n",
      "Epoch: 0281 cost= 87.648626709 latent count= 2\n",
      "Epoch: 0291 cost= 87.852652900 latent count= 2\n",
      "Epoch: 0001 cost= 210.805377586 latent count= 10\n",
      "Epoch: 0011 cost= 103.174246424 latent count= 10\n",
      "Epoch: 0021 cost= 90.123623421 latent count= 10\n",
      "Epoch: 0031 cost= 84.732490318 latent count= 10\n",
      "Epoch: 0041 cost= 81.110621636 latent count= 10\n",
      "Epoch: 0051 cost= 79.222524303 latent count= 10\n",
      "Epoch: 0061 cost= 77.866707882 latent count= 10\n",
      "Epoch: 0071 cost= 76.793867604 latent count= 10\n",
      "Epoch: 0081 cost= 75.775447922 latent count= 10\n",
      "Epoch: 0091 cost= 75.241732275 latent count= 10\n",
      "Epoch: 0101 cost= 74.519952330 latent count= 10\n",
      "Epoch: 0111 cost= 74.291633370 latent count= 10\n",
      "Epoch: 0121 cost= 73.768363842 latent count= 10\n",
      "Epoch: 0131 cost= 73.322515293 latent count= 10\n",
      "Epoch: 0141 cost= 73.448543764 latent count= 10\n",
      "Epoch: 0151 cost= 72.834045826 latent count= 10\n",
      "Epoch: 0161 cost= 72.854239169 latent count= 10\n",
      "Epoch: 0171 cost= 72.700617093 latent count= 10\n",
      "Epoch: 0181 cost= 72.251066471 latent count= 10\n",
      "Epoch: 0191 cost= 71.971827732 latent count= 10\n",
      "Epoch: 0201 cost= 71.839079305 latent count= 10\n",
      "Epoch: 0211 cost= 71.876475206 latent count= 10\n",
      "Epoch: 0221 cost= 71.389834775 latent count= 10\n",
      "Epoch: 0231 cost= 71.561263532 latent count= 10\n",
      "Epoch: 0241 cost= 71.478706512 latent count= 10\n",
      "Epoch: 0251 cost= 71.252410119 latent count= 10\n",
      "Epoch: 0261 cost= 71.199775079 latent count= 10\n",
      "Epoch: 0271 cost= 70.401531913 latent count= 10\n",
      "Epoch: 0281 cost= 70.821298114 latent count= 10\n",
      "Epoch: 0291 cost= 70.814714286 latent count= 10\n",
      "Epoch: 0001 cost= 210.067998685 latent count= 19\n",
      "Epoch: 0011 cost= 103.006424144 latent count= 20\n",
      "Epoch: 0021 cost= 91.261352220 latent count= 20\n",
      "Epoch: 0031 cost= 85.069106646 latent count= 20\n",
      "Epoch: 0041 cost= 80.804856422 latent count= 20\n",
      "Epoch: 0051 cost= 79.274147318 latent count= 20\n",
      "Epoch: 0061 cost= 77.471688600 latent count= 19\n",
      "Epoch: 0071 cost= 76.686083769 latent count= 19\n",
      "Epoch: 0081 cost= 76.225499330 latent count= 19\n",
      "Epoch: 0091 cost= 75.167838260 latent count= 17\n",
      "Epoch: 0101 cost= 74.568210047 latent count= 17\n",
      "Epoch: 0111 cost= 73.941122901 latent count= 13\n",
      "Epoch: 0121 cost= 73.745705448 latent count= 12\n",
      "Epoch: 0131 cost= 73.483014741 latent count= 14\n",
      "Epoch: 0141 cost= 72.647000656 latent count= 12\n",
      "Epoch: 0151 cost= 72.846223901 latent count= 13\n",
      "Epoch: 0161 cost= 72.225225102 latent count= 13\n",
      "Epoch: 0171 cost= 72.020193863 latent count= 12\n",
      "Epoch: 0181 cost= 72.195168540 latent count= 10\n",
      "Epoch: 0191 cost= 71.672149249 latent count= 10\n",
      "Epoch: 0201 cost= 71.510920570 latent count= 10\n",
      "Epoch: 0211 cost= 71.535581852 latent count= 10\n",
      "Epoch: 0221 cost= 71.151409614 latent count= 10\n",
      "Epoch: 0231 cost= 71.079414284 latent count= 10\n",
      "Epoch: 0241 cost= 70.809294468 latent count= 10\n",
      "Epoch: 0251 cost= 70.923012147 latent count= 10\n",
      "Epoch: 0261 cost= 70.957929805 latent count= 10\n",
      "Epoch: 0271 cost= 70.946279123 latent count= 10\n",
      "Epoch: 0281 cost= 70.358906000 latent count= 10\n",
      "Epoch: 0291 cost= 70.303651824 latent count= 10\n",
      "Epoch: 0001 cost= 212.039563182 latent count= 74\n",
      "Epoch: 0011 cost= 108.223955910 latent count= 100\n",
      "Epoch: 0021 cost= 97.557505576 latent count= 100\n",
      "Epoch: 0031 cost= 88.944950256 latent count= 100\n",
      "Epoch: 0041 cost= 83.746084636 latent count= 100\n",
      "Epoch: 0051 cost= 81.057295844 latent count= 100\n",
      "Epoch: 0061 cost= 79.630861733 latent count= 98\n",
      "Epoch: 0071 cost= 77.974867679 latent count= 97\n",
      "Epoch: 0081 cost= 76.828411331 latent count= 93\n",
      "Epoch: 0091 cost= 76.121758118 latent count= 92\n",
      "Epoch: 0101 cost= 75.618519641 latent count= 80\n",
      "Epoch: 0111 cost= 74.771400327 latent count= 76\n",
      "Epoch: 0121 cost= 74.342739799 latent count= 83\n",
      "Epoch: 0131 cost= 74.006959124 latent count= 79\n",
      "Epoch: 0141 cost= 73.529232774 latent count= 71\n",
      "Epoch: 0151 cost= 73.609742147 latent count= 67\n",
      "Epoch: 0161 cost= 73.273069271 latent count= 70\n",
      "Epoch: 0171 cost= 72.790733767 latent count= 58\n",
      "Epoch: 0181 cost= 72.879813628 latent count= 56\n",
      "Epoch: 0191 cost= 71.881549544 latent count= 59\n",
      "Epoch: 0201 cost= 71.945625590 latent count= 64\n",
      "Epoch: 0211 cost= 72.167097501 latent count= 56\n",
      "Epoch: 0221 cost= 72.083336952 latent count= 51\n",
      "Epoch: 0231 cost= 71.803392937 latent count= 61\n",
      "Epoch: 0241 cost= 71.619756130 latent count= 52\n",
      "Epoch: 0251 cost= 71.470372460 latent count= 47\n",
      "Epoch: 0261 cost= 71.244308929 latent count= 55\n",
      "Epoch: 0271 cost= 71.219139404 latent count= 54\n",
      "Epoch: 0281 cost= 71.009915348 latent count= 57\n",
      "Epoch: 0291 cost= 70.682575677 latent count= 50\n",
      "Epoch: 0001 cost= 215.450099751 latent count= 2\n",
      "Epoch: 0011 cost= 111.874371615 latent count= 2\n",
      "Epoch: 0021 cost= 105.215514887 latent count= 2\n",
      "Epoch: 0031 cost= 103.420287684 latent count= 2\n",
      "Epoch: 0041 cost= 100.980850109 latent count= 2\n",
      "Epoch: 0051 cost= 99.260739913 latent count= 2\n",
      "Epoch: 0061 cost= 97.424210136 latent count= 2\n",
      "Epoch: 0071 cost= 95.225366807 latent count= 2\n",
      "Epoch: 0081 cost= 94.359534773 latent count= 2\n",
      "Epoch: 0091 cost= 93.817408489 latent count= 2\n",
      "Epoch: 0101 cost= 93.166991376 latent count= 2\n",
      "Epoch: 0111 cost= 92.783388228 latent count= 2\n",
      "Epoch: 0121 cost= 92.065948667 latent count= 2\n",
      "Epoch: 0131 cost= 91.957425121 latent count= 2\n",
      "Epoch: 0141 cost= 91.340528981 latent count= 2\n",
      "Epoch: 0151 cost= 91.380433544 latent count= 2\n",
      "Epoch: 0161 cost= 90.992179565 latent count= 2\n",
      "Epoch: 0171 cost= 89.788801048 latent count= 2\n",
      "Epoch: 0181 cost= 90.505837805 latent count= 2\n",
      "Epoch: 0191 cost= 90.258962291 latent count= 2\n",
      "Epoch: 0201 cost= 90.176233770 latent count= 2\n",
      "Epoch: 0211 cost= 89.332228449 latent count= 2\n",
      "Epoch: 0221 cost= 89.024190993 latent count= 2\n",
      "Epoch: 0231 cost= 89.277060367 latent count= 2\n",
      "Epoch: 0241 cost= 89.070648353 latent count= 2\n",
      "Epoch: 0251 cost= 88.149202139 latent count= 2\n",
      "Epoch: 0261 cost= 88.286110770 latent count= 2\n",
      "Epoch: 0271 cost= 88.437771981 latent count= 2\n",
      "Epoch: 0281 cost= 88.222465071 latent count= 2\n",
      "Epoch: 0291 cost= 87.815303400 latent count= 2\n",
      "Epoch: 0001 cost= 209.536630263 latent count= 10\n",
      "Epoch: 0011 cost= 103.125021321 latent count= 10\n",
      "Epoch: 0021 cost= 90.027482147 latent count= 10\n",
      "Epoch: 0031 cost= 84.802057863 latent count= 10\n",
      "Epoch: 0041 cost= 81.917098208 latent count= 10\n",
      "Epoch: 0051 cost= 79.226440610 latent count= 10\n",
      "Epoch: 0061 cost= 77.549454616 latent count= 10\n",
      "Epoch: 0071 cost= 76.235084194 latent count= 10\n",
      "Epoch: 0081 cost= 75.597040898 latent count= 10\n",
      "Epoch: 0091 cost= 75.140783469 latent count= 10\n",
      "Epoch: 0101 cost= 74.654057042 latent count= 10\n",
      "Epoch: 0111 cost= 73.685483613 latent count= 10\n",
      "Epoch: 0121 cost= 73.798437368 latent count= 10\n",
      "Epoch: 0131 cost= 73.147302787 latent count= 10\n",
      "Epoch: 0141 cost= 73.229206349 latent count= 10\n",
      "Epoch: 0151 cost= 72.954485113 latent count= 10\n",
      "Epoch: 0161 cost= 72.179048157 latent count= 10\n",
      "Epoch: 0171 cost= 72.398622048 latent count= 10\n",
      "Epoch: 0181 cost= 72.215987986 latent count= 10\n",
      "Epoch: 0191 cost= 72.244502217 latent count= 10\n",
      "Epoch: 0201 cost= 71.768990520 latent count= 10\n",
      "Epoch: 0211 cost= 72.016617681 latent count= 10\n",
      "Epoch: 0221 cost= 71.813362434 latent count= 10\n",
      "Epoch: 0231 cost= 71.485436700 latent count= 10\n",
      "Epoch: 0241 cost= 71.165664936 latent count= 10\n",
      "Epoch: 0251 cost= 71.041481101 latent count= 10\n",
      "Epoch: 0261 cost= 71.027916926 latent count= 10\n",
      "Epoch: 0271 cost= 70.869654805 latent count= 10\n",
      "Epoch: 0281 cost= 70.829709764 latent count= 10\n",
      "Epoch: 0291 cost= 70.933726599 latent count= 10\n",
      "Epoch: 0001 cost= 210.092685353 latent count= 20\n",
      "Epoch: 0011 cost= 102.239934068 latent count= 20\n",
      "Epoch: 0021 cost= 90.447831490 latent count= 20\n",
      "Epoch: 0031 cost= 84.266260411 latent count= 20\n",
      "Epoch: 0041 cost= 81.199104351 latent count= 20\n",
      "Epoch: 0051 cost= 78.660148794 latent count= 20\n",
      "Epoch: 0061 cost= 77.336755565 latent count= 20\n",
      "Epoch: 0071 cost= 76.722030397 latent count= 20\n",
      "Epoch: 0081 cost= 75.332797775 latent count= 20\n",
      "Epoch: 0091 cost= 75.101793407 latent count= 20\n",
      "Epoch: 0101 cost= 74.710720492 latent count= 17\n",
      "Epoch: 0111 cost= 74.066587913 latent count= 18\n",
      "Epoch: 0121 cost= 73.510509325 latent count= 18\n",
      "Epoch: 0131 cost= 73.316177659 latent count= 17\n",
      "Epoch: 0141 cost= 73.161360564 latent count= 17\n",
      "Epoch: 0151 cost= 72.661129095 latent count= 18\n",
      "Epoch: 0161 cost= 72.472842484 latent count= 18\n",
      "Epoch: 0171 cost= 72.180159794 latent count= 18\n",
      "Epoch: 0181 cost= 72.029342735 latent count= 18\n",
      "Epoch: 0191 cost= 71.939878325 latent count= 18\n",
      "Epoch: 0201 cost= 71.387947741 latent count= 18\n",
      "Epoch: 0211 cost= 71.543899619 latent count= 18\n",
      "Epoch: 0221 cost= 71.762700064 latent count= 18\n",
      "Epoch: 0231 cost= 71.286155208 latent count= 18\n",
      "Epoch: 0241 cost= 70.917172262 latent count= 18\n",
      "Epoch: 0251 cost= 71.164464139 latent count= 18\n",
      "Epoch: 0261 cost= 70.931524062 latent count= 18\n",
      "Epoch: 0271 cost= 70.581188798 latent count= 18\n",
      "Epoch: 0281 cost= 70.736916989 latent count= 18\n",
      "Epoch: 0291 cost= 70.732705439 latent count= 18\n",
      "Epoch: 0001 cost= 212.432539645 latent count= 75\n",
      "Epoch: 0011 cost= 106.661415031 latent count= 100\n",
      "Epoch: 0021 cost= 97.972293854 latent count= 100\n",
      "Epoch: 0031 cost= 91.173120755 latent count= 100\n",
      "Epoch: 0041 cost= 84.684219874 latent count= 99\n",
      "Epoch: 0051 cost= 81.039382310 latent count= 100\n",
      "Epoch: 0061 cost= 78.619025893 latent count= 99\n",
      "Epoch: 0071 cost= 77.301961684 latent count= 97\n",
      "Epoch: 0081 cost= 76.432894717 latent count= 97\n",
      "Epoch: 0091 cost= 75.620810762 latent count= 97\n",
      "Epoch: 0101 cost= 75.196720096 latent count= 93\n",
      "Epoch: 0111 cost= 74.780581707 latent count= 94\n",
      "Epoch: 0121 cost= 73.793755993 latent count= 96\n",
      "Epoch: 0131 cost= 73.366241018 latent count= 95\n",
      "Epoch: 0141 cost= 73.472343271 latent count= 92\n",
      "Epoch: 0151 cost= 73.451728717 latent count= 95\n",
      "Epoch: 0161 cost= 72.799770716 latent count= 95\n",
      "Epoch: 0171 cost= 72.398421118 latent count= 91\n",
      "Epoch: 0181 cost= 72.381566918 latent count= 85\n",
      "Epoch: 0191 cost= 72.200465816 latent count= 86\n",
      "Epoch: 0201 cost= 72.105821533 latent count= 84\n",
      "Epoch: 0211 cost= 71.798894223 latent count= 78\n",
      "Epoch: 0221 cost= 71.743492640 latent count= 66\n",
      "Epoch: 0231 cost= 71.499660138 latent count= 53\n",
      "Epoch: 0241 cost= 71.026409968 latent count= 53\n",
      "Epoch: 0251 cost= 71.159226338 latent count= 44\n",
      "Epoch: 0261 cost= 70.931838830 latent count= 28\n",
      "Epoch: 0271 cost= 70.950761857 latent count= 27\n",
      "Epoch: 0281 cost= 70.543813213 latent count= 20\n",
      "Epoch: 0291 cost= 70.511645057 latent count= 15\n",
      "Epoch: 0001 cost= 212.718385454 latent count= 2\n",
      "Epoch: 0011 cost= 111.837614344 latent count= 2\n",
      "Epoch: 0021 cost= 105.934491341 latent count= 2\n",
      "Epoch: 0031 cost= 103.342369981 latent count= 2\n",
      "Epoch: 0041 cost= 101.218033933 latent count= 2\n",
      "Epoch: 0051 cost= 99.055409837 latent count= 2\n",
      "Epoch: 0061 cost= 97.101610315 latent count= 2\n",
      "Epoch: 0071 cost= 95.697735221 latent count= 2\n",
      "Epoch: 0081 cost= 94.732362435 latent count= 2\n",
      "Epoch: 0091 cost= 93.935048606 latent count= 2\n",
      "Epoch: 0101 cost= 93.306559864 latent count= 2\n",
      "Epoch: 0111 cost= 92.434083002 latent count= 2\n",
      "Epoch: 0121 cost= 91.738535794 latent count= 2\n",
      "Epoch: 0131 cost= 90.983874706 latent count= 2\n",
      "Epoch: 0141 cost= 90.978059970 latent count= 2\n",
      "Epoch: 0151 cost= 91.277503405 latent count= 2\n",
      "Epoch: 0161 cost= 90.365226690 latent count= 2\n",
      "Epoch: 0171 cost= 89.960031808 latent count= 2\n",
      "Epoch: 0181 cost= 90.677602511 latent count= 2\n",
      "Epoch: 0191 cost= 89.512748899 latent count= 2\n",
      "Epoch: 0201 cost= 89.685550551 latent count= 2\n",
      "Epoch: 0211 cost= 88.859341722 latent count= 2\n",
      "Epoch: 0221 cost= 88.512826191 latent count= 2\n",
      "Epoch: 0231 cost= 89.297549217 latent count= 2\n",
      "Epoch: 0241 cost= 88.554633442 latent count= 2\n",
      "Epoch: 0251 cost= 88.516442552 latent count= 2\n",
      "Epoch: 0261 cost= 88.876015126 latent count= 2\n",
      "Epoch: 0271 cost= 88.177450950 latent count= 2\n",
      "Epoch: 0281 cost= 88.228089946 latent count= 2\n",
      "Epoch: 0291 cost= 88.187583715 latent count= 2\n",
      "Epoch: 0001 cost= 215.617831615 latent count= 10\n",
      "Epoch: 0011 cost= 104.174358687 latent count= 10\n",
      "Epoch: 0021 cost= 90.723232963 latent count= 10\n",
      "Epoch: 0031 cost= 83.916173900 latent count= 10\n",
      "Epoch: 0041 cost= 80.671776068 latent count= 10\n",
      "Epoch: 0051 cost= 79.230536880 latent count= 10\n",
      "Epoch: 0061 cost= 77.366654080 latent count= 10\n",
      "Epoch: 0071 cost= 77.028148908 latent count= 10\n",
      "Epoch: 0081 cost= 75.911947722 latent count= 10\n",
      "Epoch: 0091 cost= 75.216251165 latent count= 10\n",
      "Epoch: 0101 cost= 74.719398665 latent count= 10\n",
      "Epoch: 0111 cost= 74.511158399 latent count= 10\n",
      "Epoch: 0121 cost= 74.124503285 latent count= 10\n",
      "Epoch: 0131 cost= 73.333395157 latent count= 9\n",
      "Epoch: 0141 cost= 73.735457354 latent count= 10\n",
      "Epoch: 0151 cost= 73.277265452 latent count= 10\n",
      "Epoch: 0161 cost= 72.776241878 latent count= 9\n",
      "Epoch: 0171 cost= 72.784705720 latent count= 10\n",
      "Epoch: 0181 cost= 72.173475800 latent count= 9\n",
      "Epoch: 0191 cost= 72.242744612 latent count= 9\n",
      "Epoch: 0201 cost= 72.190374451 latent count= 9\n",
      "Epoch: 0211 cost= 71.966746271 latent count= 9\n",
      "Epoch: 0221 cost= 71.588015081 latent count= 9\n",
      "Epoch: 0231 cost= 71.476344383 latent count= 9\n",
      "Epoch: 0241 cost= 71.692231140 latent count= 9\n",
      "Epoch: 0251 cost= 71.327293888 latent count= 9\n",
      "Epoch: 0261 cost= 71.238750687 latent count= 9\n",
      "Epoch: 0271 cost= 71.003138511 latent count= 9\n",
      "Epoch: 0281 cost= 70.935389869 latent count= 9\n",
      "Epoch: 0291 cost= 70.941014855 latent count= 9\n",
      "Epoch: 0001 cost= 209.918592640 latent count= 19\n",
      "Epoch: 0011 cost= 105.540004994 latent count= 20\n",
      "Epoch: 0021 cost= 90.825662710 latent count= 20\n",
      "Epoch: 0031 cost= 85.034576326 latent count= 20\n",
      "Epoch: 0041 cost= 81.772021672 latent count= 20\n",
      "Epoch: 0051 cost= 79.396352407 latent count= 20\n",
      "Epoch: 0061 cost= 78.023192631 latent count= 20\n",
      "Epoch: 0071 cost= 77.016654920 latent count= 20\n",
      "Epoch: 0081 cost= 75.978140522 latent count= 20\n",
      "Epoch: 0091 cost= 75.122258655 latent count= 20\n",
      "Epoch: 0101 cost= 74.413801124 latent count= 19\n",
      "Epoch: 0111 cost= 74.081545930 latent count= 19\n",
      "Epoch: 0121 cost= 73.620060820 latent count= 19\n",
      "Epoch: 0131 cost= 73.427352690 latent count= 19\n",
      "Epoch: 0141 cost= 73.456763007 latent count= 18\n",
      "Epoch: 0151 cost= 72.858735185 latent count= 17\n",
      "Epoch: 0161 cost= 72.370849027 latent count= 17\n",
      "Epoch: 0171 cost= 72.227937338 latent count= 17\n",
      "Epoch: 0181 cost= 72.284141707 latent count= 15\n",
      "Epoch: 0191 cost= 71.576874688 latent count= 16\n",
      "Epoch: 0201 cost= 71.678749945 latent count= 14\n",
      "Epoch: 0211 cost= 71.626554669 latent count= 13\n",
      "Epoch: 0221 cost= 71.391975951 latent count= 12\n",
      "Epoch: 0231 cost= 71.113452731 latent count= 13\n",
      "Epoch: 0241 cost= 71.012729125 latent count= 13\n",
      "Epoch: 0251 cost= 71.026259398 latent count= 10\n",
      "Epoch: 0261 cost= 70.618468656 latent count= 12\n",
      "Epoch: 0271 cost= 70.635390396 latent count= 10\n",
      "Epoch: 0281 cost= 70.579323633 latent count= 11\n",
      "Epoch: 0291 cost= 70.491039269 latent count= 10\n",
      "Epoch: 0001 cost= 212.753478699 latent count= 64\n",
      "Epoch: 0011 cost= 107.604742473 latent count= 100\n",
      "Epoch: 0021 cost= 97.681429804 latent count= 100\n",
      "Epoch: 0031 cost= 89.482275460 latent count= 100\n",
      "Epoch: 0041 cost= 84.570606488 latent count= 99\n",
      "Epoch: 0051 cost= 81.034998779 latent count= 96\n",
      "Epoch: 0061 cost= 79.066298377 latent count= 88\n",
      "Epoch: 0071 cost= 77.233700222 latent count= 78\n",
      "Epoch: 0081 cost= 76.281155396 latent count= 78\n",
      "Epoch: 0091 cost= 75.785253046 latent count= 77\n",
      "Epoch: 0101 cost= 75.132803907 latent count= 61\n",
      "Epoch: 0111 cost= 74.650223021 latent count= 65\n",
      "Epoch: 0121 cost= 73.792054853 latent count= 75\n",
      "Epoch: 0131 cost= 73.521088749 latent count= 69\n",
      "Epoch: 0141 cost= 73.204828755 latent count= 69\n",
      "Epoch: 0151 cost= 72.842147418 latent count= 69\n",
      "Epoch: 0161 cost= 72.413149518 latent count= 69\n",
      "Epoch: 0171 cost= 72.700303393 latent count= 69\n",
      "Epoch: 0181 cost= 72.394207202 latent count= 71\n",
      "Epoch: 0191 cost= 72.215972602 latent count= 59\n",
      "Epoch: 0201 cost= 72.057986457 latent count= 60\n",
      "Epoch: 0211 cost= 71.954963629 latent count= 56\n",
      "Epoch: 0221 cost= 71.275599428 latent count= 54\n",
      "Epoch: 0231 cost= 71.187797741 latent count= 52\n",
      "Epoch: 0241 cost= 71.289970051 latent count= 51\n",
      "Epoch: 0251 cost= 71.113616527 latent count= 51\n",
      "Epoch: 0261 cost= 70.969560249 latent count= 53\n",
      "Epoch: 0271 cost= 71.133015206 latent count= 51\n",
      "Epoch: 0281 cost= 70.749364680 latent count= 51\n",
      "Epoch: 0291 cost= 70.884250627 latent count= 49\n",
      "Epoch: 0001 cost= 222.757187888 latent count= 1\n",
      "Epoch: 0011 cost= 127.934232081 latent count= 41\n",
      "Epoch: 0021 cost= 116.262558621 latent count= 46\n",
      "Epoch: 0031 cost= 112.516969341 latent count= 45\n",
      "Epoch: 0041 cost= 105.447522860 latent count= 36\n",
      "Epoch: 0051 cost= 99.326435797 latent count= 5\n",
      "Epoch: 0061 cost= 96.727176195 latent count= 3\n",
      "Epoch: 0071 cost= 95.643119673 latent count= 3\n",
      "Epoch: 0081 cost= 94.759164026 latent count= 3\n",
      "Epoch: 0091 cost= 93.421214086 latent count= 3\n",
      "Epoch: 0101 cost= 93.118396773 latent count= 3\n",
      "Epoch: 0111 cost= 91.733750125 latent count= 3\n",
      "Epoch: 0121 cost= 91.547096682 latent count= 3\n",
      "Epoch: 0131 cost= 90.852358426 latent count= 3\n",
      "Epoch: 0141 cost= 89.987774298 latent count= 3\n",
      "Epoch: 0151 cost= 89.882103382 latent count= 3\n",
      "Epoch: 0161 cost= 89.950220073 latent count= 3\n",
      "Epoch: 0171 cost= 89.945752383 latent count= 3\n",
      "Epoch: 0181 cost= 89.775631755 latent count= 3\n",
      "Epoch: 0191 cost= 89.193769989 latent count= 3\n",
      "Epoch: 0201 cost= 88.886144950 latent count= 3\n",
      "Epoch: 0211 cost= 88.807851056 latent count= 3\n",
      "Epoch: 0221 cost= 88.648200420 latent count= 3\n",
      "Epoch: 0231 cost= 88.495807460 latent count= 3\n",
      "Epoch: 0241 cost= 88.108483498 latent count= 3\n",
      "Epoch: 0251 cost= 88.171744537 latent count= 3\n",
      "Epoch: 0261 cost= 88.022076915 latent count= 3\n",
      "Epoch: 0271 cost= 88.389316628 latent count= 3\n",
      "Epoch: 0281 cost= 88.096368866 latent count= 3\n",
      "Epoch: 0291 cost= 87.450874773 latent count= 3\n",
      "Epoch: 0001 cost= 215.932498821 latent count= 23\n",
      "Epoch: 0011 cost= 117.533924463 latent count= 49\n",
      "Epoch: 0021 cost= 106.952733432 latent count= 49\n",
      "Epoch: 0031 cost= 101.602334026 latent count= 32\n",
      "Epoch: 0041 cost= 96.304286568 latent count= 17\n",
      "Epoch: 0051 cost= 92.634222495 latent count= 18\n",
      "Epoch: 0061 cost= 89.865875008 latent count= 6\n",
      "Epoch: 0071 cost= 88.472686268 latent count= 4\n",
      "Epoch: 0081 cost= 87.656561987 latent count= 4\n",
      "Epoch: 0091 cost= 86.782123191 latent count= 4\n",
      "Epoch: 0101 cost= 86.628337250 latent count= 4\n",
      "Epoch: 0111 cost= 86.002467561 latent count= 4\n",
      "Epoch: 0121 cost= 84.914003636 latent count= 4\n",
      "Epoch: 0131 cost= 85.173749792 latent count= 4\n",
      "Epoch: 0141 cost= 84.254872846 latent count= 4\n",
      "Epoch: 0151 cost= 84.001924799 latent count= 4\n",
      "Epoch: 0161 cost= 83.923768089 latent count= 4\n",
      "Epoch: 0171 cost= 83.912054596 latent count= 4\n",
      "Epoch: 0181 cost= 83.326981028 latent count= 4\n",
      "Epoch: 0191 cost= 83.420322023 latent count= 4\n",
      "Epoch: 0201 cost= 83.380946912 latent count= 4\n",
      "Epoch: 0211 cost= 83.368772098 latent count= 4\n",
      "Epoch: 0221 cost= 82.742033997 latent count= 4\n",
      "Epoch: 0231 cost= 82.890425859 latent count= 4\n",
      "Epoch: 0241 cost= 82.392785554 latent count= 4\n",
      "Epoch: 0251 cost= 82.170042073 latent count= 4\n",
      "Epoch: 0261 cost= 82.288646018 latent count= 4\n",
      "Epoch: 0271 cost= 82.176852056 latent count= 4\n",
      "Epoch: 0281 cost= 81.738448708 latent count= 4\n",
      "Epoch: 0291 cost= 81.451610648 latent count= 4\n",
      "Epoch: 0001 cost= 216.885617384 latent count= 39\n",
      "Epoch: 0011 cost= 112.561045047 latent count= 50\n",
      "Epoch: 0021 cost= 102.510857294 latent count= 49\n",
      "Epoch: 0031 cost= 95.504364707 latent count= 50\n",
      "Epoch: 0041 cost= 91.239646676 latent count= 49\n",
      "Epoch: 0051 cost= 87.884693326 latent count= 42\n",
      "Epoch: 0061 cost= 85.283671272 latent count= 37\n",
      "Epoch: 0071 cost= 83.598497717 latent count= 24\n",
      "Epoch: 0081 cost= 81.957781545 latent count= 22\n",
      "Epoch: 0091 cost= 80.833081159 latent count= 13\n",
      "Epoch: 0101 cost= 80.050587172 latent count= 8\n",
      "Epoch: 0111 cost= 79.282015915 latent count= 8\n",
      "Epoch: 0121 cost= 78.966220349 latent count= 7\n",
      "Epoch: 0131 cost= 78.204503784 latent count= 7\n",
      "Epoch: 0141 cost= 78.046435263 latent count= 7\n",
      "Epoch: 0151 cost= 77.833951159 latent count= 7\n",
      "Epoch: 0161 cost= 77.432657616 latent count= 7\n",
      "Epoch: 0171 cost= 77.497602095 latent count= 7\n",
      "Epoch: 0181 cost= 76.976992139 latent count= 7\n",
      "Epoch: 0191 cost= 76.469163465 latent count= 7\n",
      "Epoch: 0201 cost= 76.431412104 latent count= 7\n",
      "Epoch: 0211 cost= 76.225035650 latent count= 7\n",
      "Epoch: 0221 cost= 76.379537319 latent count= 7\n",
      "Epoch: 0231 cost= 76.010274027 latent count= 7\n",
      "Epoch: 0241 cost= 76.402307233 latent count= 7\n",
      "Epoch: 0251 cost= 75.854084230 latent count= 7\n",
      "Epoch: 0261 cost= 75.738521097 latent count= 7\n",
      "Epoch: 0271 cost= 75.835038896 latent count= 7\n",
      "Epoch: 0281 cost= 74.913421000 latent count= 7\n",
      "Epoch: 0291 cost= 75.565117722 latent count= 7\n",
      "Epoch: 0001 cost= 227.600414776 latent count= 2\n",
      "Epoch: 0011 cost= 127.733113514 latent count= 35\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print i\n",
    "    #Vanilla\n",
    "    print 'VANILLA'\n",
    "    run_test(50,1.0,0,0,0,trial_num=i+2)\n",
    "    #IW\n",
    "    print 'IW' \n",
    "    run_test(50,1.0,0,0,1,trial_num=i+2)\n",
    "    #WU\n",
    "    print 'WU' \n",
    "    run_test(50,1.0,0,1,1,trial_num=i+2)\n",
    "    \n",
    "for i in range(3):\n",
    "    for nz in [2,10,20,100]:\n",
    "        run_test(nz,1.0,0,0,1,trial_num=i)\n",
    "        \n",
    "for i in range(3):\n",
    "    for keep in [0.6,0.8,0.9]:\n",
    "        run_test(50,keep,0,0,1,trial_num=i)\n",
    "        \n",
    "for i in range(5):\n",
    "    print 'BN' \n",
    "    run_test(50,1.0,1,0,1,trial_num=i+2)\n",
    "    #BN+WU\n",
    "    print 'BNWU' \n",
    "    run_test(50,1.0,1,1,1,trial_num=i+2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print i\n",
    "    #Vanilla\n",
    "    print 'VANILLA'\n",
    "    run_test(50,1.0,0,1,1,trial_num=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 207.612559690 latent count= 37\n",
      "Epoch: 0011 cost= 105.542976532 latent count= 50\n",
      "Epoch: 0021 cost= 94.349551239 latent count= 50\n",
      "Epoch: 0031 cost= 87.164091270 latent count= 50\n",
      "Epoch: 0041 cost= 82.547086085 latent count= 50\n",
      "Epoch: 0051 cost= 79.064451599 latent count= 50\n",
      "Epoch: 0061 cost= 77.767465376 latent count= 47\n",
      "Epoch: 0071 cost= 76.413862749 latent count= 48\n",
      "Epoch: 0081 cost= 75.176812231 latent count= 42\n",
      "Epoch: 0091 cost= 74.372862646 latent count= 38\n",
      "Epoch: 0101 cost= 73.988233851 latent count= 35\n",
      "Epoch: 0111 cost= 73.509260490 latent count= 35\n",
      "Epoch: 0121 cost= 73.345513715 latent count= 25\n",
      "Epoch: 0131 cost= 72.751592664 latent count= 21\n",
      "Epoch: 0141 cost= 72.273427055 latent count= 23\n",
      "Epoch: 0151 cost= 72.261220259 latent count= 18\n",
      "Epoch: 0161 cost= 71.838041486 latent count= 14\n",
      "Epoch: 0171 cost= 72.133169923 latent count= 13\n",
      "Epoch: 0181 cost= 71.512687052 latent count= 13\n",
      "Epoch: 0191 cost= 71.357833051 latent count= 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError() in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7f080708cc10>> ignored\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-631a2d4a7d2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrial_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-773b822e2ef3>\u001b[0m in \u001b[0;36mrun_test\u001b[1;34m(z_dim, keep_prob, b_normal, warmup, iw, t_epochs, trial_num)\u001b[0m\n\u001b[0;32m     14\u001b[0m                                         \u001b[0mbatch_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mb_normal\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                                         \u001b[0mwarmup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwarmup\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                                         iw=iw)\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mnamestring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'trials_newobj/trial_num.{}.{}.{}.{}.{}.{}.pkl'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial_num\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb_normal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwarmup\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'cost'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcost_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'covar'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcovar_list\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnamestring\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-30bd66ec9cfa>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(network_architecture, learning_rate, batch_size, training_epochs, display_step, train_keep_prob, batch_norm, warmup, iw)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;31m# Fit training using batch data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[1;31m# Compute average loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mavg_cost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcost\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-caaef2b5202e>\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, epo)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         opt, cost = self.sess.run((self.optimizer, self.cost), \n\u001b[1;32m--> 337\u001b[1;33m                                   feed_dict={self.x: X, self.keep_prob: self.train_keep_prob})\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 343\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    344\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    565\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 567\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    568\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m       \u001b[1;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    638\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 640\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    641\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    645\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m       \u001b[0merror_message\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    629\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m--> 631\u001b[1;33m             session, None, feed_dict, fetch_list, target_list, None)\n\u001b[0m\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_test(50,1.0,0,1,1,trial_num=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 238.594773643 latent count= 48\n"
     ]
    }
   ],
   "source": [
    "network_architecture = \\\n",
    "    dict(n_hidden_recog_1=200, # 1st layer encoder neurons\n",
    "         n_hidden_recog_2=200, # 2nd layer encoder neurons\n",
    "         n_hidden_gener_1=200, # 1st layer decoder neurons\n",
    "         n_hidden_gener_2=200, # 2nd layer decoder neurons\n",
    "         n_input=784, # MNIST data input (img shape: 28*28)\n",
    "         n_z=50)  # dimensionality of latent space\n",
    "\n",
    "vae, cost_list, covar_list = train(network_architecture,learning_rate=0.001,\n",
    "                                    batch_size=100,\n",
    "                                    training_epochs=1,\n",
    "                                    display_step=10,\n",
    "                                    train_keep_prob=1.0,\n",
    "                                    batch_norm = 0,\n",
    "                                    warmup = 0,\n",
    "                                    iw = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-bc5b6e97c30f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 343\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    344\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    518\u001b[0m     \u001b[1;31m# Check session.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Attempted to use a closed Session.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 199.594517767 latent count= 47\n",
      "Epoch: 0011 cost= 116.061458102 latent count= 9\n",
      "Epoch: 0021 cost= 111.486567480 latent count= 9\n",
      "Epoch: 0031 cost= 109.708253063 latent count= 9\n",
      "Epoch: 0041 cost= 108.606147641 latent count= 9\n",
      "Epoch: 0051 cost= 107.887856501 latent count= 9\n",
      "Epoch: 0061 cost= 107.359480397 latent count= 9\n",
      "Epoch: 0071 cost= 106.920686132 latent count= 9\n",
      "Epoch: 0081 cost= 106.604738506 latent count= 9\n",
      "Epoch: 0091 cost= 106.291280642 latent count= 9\n",
      "Epoch: 0101 cost= 106.015755088 latent count= 9\n",
      "Epoch: 0111 cost= 105.791387801 latent count= 9\n",
      "Epoch: 0121 cost= 105.613634768 latent count= 9\n",
      "Epoch: 0131 cost= 105.327047161 latent count= 9\n",
      "Epoch: 0141 cost= 105.306643080 latent count= 9\n",
      "Epoch: 0151 cost= 105.104632596 latent count= 9\n",
      "Epoch: 0161 cost= 104.945250022 latent count= 9\n",
      "Epoch: 0171 cost= 104.801097273 latent count= 9\n",
      "Epoch: 0181 cost= 104.717475891 latent count= 9\n",
      "Epoch: 0191 cost= 104.577905690 latent count= 9\n",
      "Epoch: 0201 cost= 104.493117648 latent count= 9\n",
      "Epoch: 0211 cost= 104.363819164 latent count= 9\n",
      "Epoch: 0221 cost= 104.289365886 latent count= 9\n",
      "Epoch: 0231 cost= 104.209707919 latent count= 9\n",
      "Epoch: 0241 cost= 104.147031111 latent count= 9\n",
      "Epoch: 0251 cost= 104.108155850 latent count= 9\n",
      "Epoch: 0261 cost= 103.979424189 latent count= 9\n",
      "Epoch: 0271 cost= 103.917594216 latent count= 9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-080f51b986d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m77777\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# for i in range(5):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#     run_test(50,0.9,0,0, trial_num=(i))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#     run_test(50,0.9,1,0, trial_num=(i))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-ba0783da2adc>\u001b[0m in \u001b[0;36mrun_test\u001b[1;34m(z_dim, keep_prob, b_normal, warmup, t_epochs, trial_num)\u001b[0m\n\u001b[0;32m     13\u001b[0m                                         \u001b[0mtrain_keep_prob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                                         \u001b[0mbatch_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mb_normal\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                                         warmup = warmup)\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mnamestring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'trials_long/trial_num.{}.{}.{}.{}.{}.pkl'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial_num\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb_normal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwarmup\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'cost'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcost_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'covar'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcovar_list\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnamestring\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-3a7d6b0f1404>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(network_architecture, learning_rate, batch_size, training_epochs, display_step, train_keep_prob, batch_norm, warmup)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;31m# Fit training using batch data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[1;31m# Compute average loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mavg_cost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcost\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-8b6b4fdc2566>\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, epo)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         opt, cost = self.sess.run((self.optimizer, self.cost), \n\u001b[1;32m--> 196\u001b[1;33m                                   feed_dict={self.x: X, self.keep_prob: self.train_keep_prob})\n\u001b[0m\u001b[0;32m    197\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 343\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    344\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    565\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 567\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    568\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m       \u001b[1;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    638\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 640\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    641\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    645\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m       \u001b[0merror_message\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    629\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m--> 631\u001b[1;33m             session, None, feed_dict, fetch_list, target_list, None)\n\u001b[0m\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_test(50,0.9,0,0, trial_num=(77777))\n",
    "\n",
    "# for i in range(5):\n",
    "#     run_test(50,0.9,0,0, trial_num=(i))\n",
    "#     run_test(50,0.9,1,0, trial_num=(i))\n",
    "#     run_test(50,0.9,1,1, trial_num=(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     run_test(50,1.0,'gaussian',0,0, trial_num=i)\n",
    "# for i in range(1):\n",
    "#     run_test(50,1.0,'bernoulli',0,0, trial_num=i)\n",
    "for i in range(5):\n",
    "    run_test(50,0.95,1,0, trial_num=(i))\n",
    "# for i in range(5):\n",
    "#     run_test(50,1.0,1,1, trial_num=i)\n",
    "    \n",
    "# # #Test dropout rate\n",
    "# for i in range(5):\n",
    "#     for keep in [0.9,0.8,0.7,0.6]:\n",
    "#         run_test(50,keep,1,1,trial_num=i)\n",
    "#Test N_z\n",
    "# for i in range(5):\n",
    "#     for n_z in [2,5,10,20,100]:\n",
    "#         run_test(n_z,0.9,1,1, trial_num=i)\n",
    "# #Test warmup over dimensions\n",
    "# for dim in [5,20,50,100]:\n",
    "#     run_test(dim,1.0,'bernoulli',1,0, trial_num=i)\n",
    "#     run_test(dim,1.0,'bernoulli',1,1, trial_num=i)\n",
    "# #Test batch norm over dimensions\n",
    "# for dim in [5,20,50,100]:\n",
    "#     run_test(dim,1.0,'bernoulli',0,1, trial_num=i)\n",
    "#     run_test(dim,1.0,'bernoulli',1,1, trial_num=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_test(50,1.0,1,0, trial_num=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_sample = mnist.test.next_batch(100)[0]\n",
    "x_reconstruct = vae.reconstruct(x_sample)\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "for i in range(5):\n",
    "\n",
    "    plt.subplot(5, 2, 2*i + 1)\n",
    "    plt.imshow(x_sample[i].reshape(28, 28), vmin=0, vmax=1)\n",
    "    plt.title(\"Test input\")\n",
    "    plt.colorbar()\n",
    "    plt.subplot(5, 2, 2*i + 2)\n",
    "    plt.imshow(x_reconstruct[i].reshape(28, 28), vmin=0, vmax=1)\n",
    "    plt.title(\"Reconstruction\")\n",
    "    plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar instance at 0x7f2e1032a4d0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc4AAAFwCAYAAAAxJpu1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGYvOkQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2e105cdd50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "x_sample, y_sample = mnist.test.next_batch(5000)\n",
    "z_mu = vae.transform(x_sample)\n",
    "plt.figure(figsize=(8, 6)) \n",
    "plt.scatter(z_mu[:, 0], z_mu[:, 1], c=np.argmax(y_sample, 1))\n",
    "plt.colorbar()\n",
    "# plt.savefig('vis1.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1, 2) for Tensor u'Add_4:0', which has shape '(100, 50)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-f3e237dee9fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mz_mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mxi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mx_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_mu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mcanvas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_mean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-8b6b4fdc2566>\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, z_mu)\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;31m# sample from Gaussian distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         return self.sess.run(self.x_reconstr_mean, \n\u001b[1;32m--> 217\u001b[1;33m                              feed_dict={self.z: z_mu, self.keep_prob: 1.0})\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 343\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    344\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    554\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m    557\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (1, 2) for Tensor u'Add_4:0', which has shape '(100, 50)'"
     ]
    }
   ],
   "source": [
    "nx = ny = 20\n",
    "x_values = np.linspace(-3, 3, nx)\n",
    "y_values = np.linspace(-3, 3, ny)\n",
    "\n",
    "canvas = np.empty((28*ny, 28*nx))\n",
    "for i, yi in enumerate(x_values):\n",
    "    for j, xi in enumerate(y_values):\n",
    "        z_mu = np.array([[xi, yi]])\n",
    "        x_mean = vae.generate(z_mu)\n",
    "        canvas[(nx-i-1)*28:(nx-i)*28, j*28:(j+1)*28] = x_mean[0].reshape(28, 28)\n",
    "\n",
    "plt.figure(figsize=(8, 10))        \n",
    "Xi, Yi = np.meshgrid(x_values, y_values)\n",
    "plt.imshow(canvas, origin=\"upper\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig('vis2.png',dpi=300)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_test(50,1.0,'bernoulli',0,0, trial_num=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_test(50,1.0,'bernoulli',0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    #Initial tests to demonstrate features\n",
    "    run_test(50,1.0,'gaussian',0,0, trial_num=i)\n",
    "    run_test(50,1.0,'bernoulli',0,0, trial_num=i)\n",
    "    run_test(50,1.0,'bernoulli',1,0, trial_num=i)\n",
    "    run_test(50,1.0,'bernoulli',1,1, trial_num=i)\n",
    "    #Test dropout rate\n",
    "    for keep in [1.0,0.9,0.8,0.7,0.6]:\n",
    "        run_test(50,keep,'bernoulli',1,1, trial_num=i)\n",
    "    #Test N_z\n",
    "    for n_z in [2,5,10,20,50,100]:\n",
    "        run_test(n_z,1.0,'bernoulli',1,1, trial_num=i)\n",
    "    #Test warmup over dimensions\n",
    "    for dim in [5,20,50,100]:\n",
    "        run_test(dim,1.0,'bernoulli',1,0, trial_num=i)\n",
    "        run_test(dim,1.0,'bernoulli',1,1, trial_num=i)\n",
    "    #Test batch norm over dimensions\n",
    "    for dim in [5,20,50,100]:\n",
    "        run_test(dim,1.0,'bernoulli',0,1, trial_num=i)\n",
    "        run_test(dim,1.0,'bernoulli',1,1, trial_num=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_test(50,0.5,'bernoulli',0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_test(50,1,'bernoulli',1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_test(50,1,'bernoulli',1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_test(50,1,'bernoulli',0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
